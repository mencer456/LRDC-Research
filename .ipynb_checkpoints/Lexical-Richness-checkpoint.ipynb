{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data analysis for Lexical Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary search for strings\n",
    "def binarySearchArr(arr, x):\n",
    "    l = 0\n",
    "    r = len(arr)\n",
    "    while (l <= r):\n",
    "        m = l + ((r - l) // 2)\n",
    " \n",
    "        res = (x == arr[m])\n",
    "        # Check if x is present at mid\n",
    "        if (res == True):\n",
    "            return m\n",
    " \n",
    "        # If x greater, ignore left half\n",
    "    #problem is here\n",
    "        elif (res == False):\n",
    "            if (x>arr[m]):\n",
    "                l = m+1\n",
    "            elif (x<arr[m]):\n",
    "                r=m-1\n",
    "        # If x is smaller, ignore right half\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function removes all of the rows that contain a string in the passed column\n",
    "def rmStr (df, col):\n",
    "    row_index=0\n",
    "    row_ind=[]\n",
    "    for i in df[col]:\n",
    "        try:\n",
    "            int(i)\n",
    "        except:\n",
    "            row_ind.append(row_index)\n",
    "        row_index+=1\n",
    "    return df.drop(labels=row_ind,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the unique values in a list\n",
    "def get_uniques(x):\n",
    "    return list(x.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the number of texts per language in the given list of anon_ids\n",
    "def find_counts(df,anon_list,lang_col):\n",
    "    spanish=0\n",
    "    chinese=0\n",
    "    korean=0\n",
    "    index=0\n",
    "    for i in df[\"Number of Questions\"]:\n",
    "        anon_id=str(df.at[index,'anon_id'])\n",
    "        if anon_id in anon_list:\n",
    "            if df.at[index,lang_col]=='Spanish':            \n",
    "                spanish+=i\n",
    "            elif df.at[index,lang_col]=='Chinese':\n",
    "                chinese+=i\n",
    "            elif df.at[index,lang_col]=='Korean':\n",
    "                korean+=i\n",
    "        index+=1\n",
    "    return(spanish,chinese,korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenAvg(df,textlencol,lang, id_list,lvl):\n",
    "    index=0\n",
    "    spanish=0\n",
    "    chinese=0\n",
    "    korean=0\n",
    "    qcounts=0\n",
    "    qcountc=0\n",
    "    qcountk=0\n",
    "\n",
    "    for anon in df['anon_id']:\n",
    "        if df.at[index,'level_id']==lvl and df.at[index,'anon_id'] in id_list:\n",
    "            textlen=df.at[index,textlencol]\n",
    "            textlen=int(textlen)\n",
    "            if df.at[index,lang]=='Spanish':\n",
    "                spanish+=textlen\n",
    "                qcounts+=1\n",
    "            elif df.at[index,lang]=='Korean':\n",
    "                korean+=textlen\n",
    "                qcountk+=1\n",
    "            elif df.at[index,lang]=='Chinese':\n",
    "                chinese+=textlen\n",
    "                qcountc+=1\n",
    "        index+=1\n",
    "        \n",
    "    print(\"Spanish Average: \",spanish/qcounts)\n",
    "    print(\"Korean Average: \",korean/qcountk)\n",
    "    print(\"Chinese Average: \",chinese/qcountc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(df,textlencol,typelencol,lang, id_list,lvl):\n",
    "    index=0\n",
    "    spanish=0\n",
    "    chinese=0\n",
    "    korean=0\n",
    "    tspanish=0\n",
    "    tkorean=0\n",
    "    tchinese=0\n",
    "    qcounts=0\n",
    "    qcountc=0\n",
    "    qcountk=0\n",
    "\n",
    "    for anon in df['anon_id']:\n",
    "        if df.at[index,'level_id']==lvl and df.at[index,'anon_id'] in id_list:\n",
    "            textlen=df.at[index,textlencol]\n",
    "            textlen=int(textlen)\n",
    "            typelen=df.at[index,typelencol]\n",
    "            typelen=int(typelen)\n",
    "            if df.at[index,lang]=='Spanish':\n",
    "                spanish+=textlen\n",
    "                tspanish+=typelen\n",
    "                qcounts+=1\n",
    "            elif df.at[index,lang]=='Korean':\n",
    "                korean+=textlen\n",
    "                tkorean+=typelen\n",
    "                qcountk+=1\n",
    "            elif df.at[index,lang]=='Chinese':\n",
    "                chinese+=textlen\n",
    "                tchinese+=typelen\n",
    "                qcountc+=1\n",
    "        index+=1\n",
    "        \n",
    "    spanishavg=spanish/qcounts\n",
    "    tspanishavg=tspanish/qcounts\n",
    "    koreanavg=korean/qcountk\n",
    "    tkoreanavg=tkorean/qcountk\n",
    "    chineseavg=chinese/qcountc\n",
    "    tchineseavg=tchinese/qcountc\n",
    "    \n",
    "    print(\"Spanish Ratio: \",tspanishavg/spanishavg)\n",
    "    print(\"Korean Ratio: \",tkoreanavg/koreanavg)\n",
    "    print(\"Chinese Ratio: \",tchineseavg/chineseavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that counts the amount of tokens per language FINDS AVERAGE OF ALL LVLS IN SPECIFIED LIST\n",
    "def finding_tokens(df,id_list,lang_col,text_len_col,qcounts):\n",
    "    spanish=0\n",
    "    korean=0\n",
    "    chinese=0\n",
    "    index=0\n",
    "    lspanish,lchinese,lkorean=find_countslvl(qcounts, id_list,lang_col)\n",
    "\n",
    "    for i in df['anon_id']:\n",
    "        if i in id_list:\n",
    "            lang=df.at[index,lang_col]\n",
    "            length=int(df.at[index,text_len_col])\n",
    "            if lang=='Spanish':\n",
    "                spanish+=length\n",
    "            elif lang=='Korean':\n",
    "                korean+=length\n",
    "            elif lang=='Chinese':\n",
    "                chinese+=length\n",
    "        index+=1\n",
    "   # print('Spanish Token Average: ',spanish,'\\nKorean Token Average: ',korean,\n",
    "    #      '\\nChinese Token Average:',chinese)\n",
    "    print('Spanish Token Average: ',spanish/lspanish,'\\nKorean Token Average: ',korean/lkorean,\n",
    "          '\\nChinese Token Average:',chinese/lchinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose=pd.read_csv('original-sheets\\Responses-Loose.csv')\n",
    "student_info=pd.read_csv('corpus-files\\student_information.csv')\n",
    "course=pd.read_csv('corpus-files\\course.csv')\n",
    "answer_df=pd.read_csv('answer.csv',index_col = 'answer_id', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_=course.drop(columns=['class_id','semester','section'])\n",
    "\n",
    "student=student_info.drop(columns=['birth_year','language_used_at_home','non_native_language_1','yrs_of_study_lang1',\n",
    "                                  'study_in_classroom_lang1','ways_of_study_lang1','non_native_language_2','study_in_classroom_lang2',\n",
    "                                  'ways_of_study_lang2','non_native_language_3','yrs_of_study_lang3','study_in_classroom_lang3',\n",
    "                                  'ways_of_study_lang3','yrs_of_english_learning','yrs_in_english_environment','yrs_of_study_lang2'])\n",
    "answer_=answer_df[['question_id','anon_id','course_id','version','text_len','text','tokens']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3Langs</th>\n",
       "      <th>Item</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text1_len</th>\n",
       "      <th>text2_len</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2 (line breaks/extra spaces removed, spaces added to reach 60)</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "      <th>Judgement</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>C12</td>\n",
       "      <td>141.0</td>\n",
       "      <td>aj8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>\\n\\n\\n Today, I am going to describe one of my...</td>\n",
       "      <td>Today, I am going to describe one of my classm...</td>\n",
       "      <td>Today, I am going to describe one of my classm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>C13</td>\n",
       "      <td>143.0</td>\n",
       "      <td>az8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Female</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>K9</td>\n",
       "      <td>133.0</td>\n",
       "      <td>az2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>K11</td>\n",
       "      <td>135.0</td>\n",
       "      <td>at8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>K20</td>\n",
       "      <td>188.0</td>\n",
       "      <td>eh9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  3Langs Item  answer_id anon_id       L1  gender  course_id  level_id  \\\n",
       "0      Y  C12      141.0     aj8  Chinese    Male      118.0       5.0   \n",
       "1      Y  C13      143.0     az8  Chinese  Female      118.0       5.0   \n",
       "2      Y   K9      133.0     az2   Korean    Male      118.0       5.0   \n",
       "3      Y  K11      135.0     at8   Korean  Female      118.0       5.0   \n",
       "4      Y  K20      188.0     eh9   Korean    Male      118.0       5.0   \n",
       "\n",
       "  class_id  question_id  version  text1_len  text2_len  text3_len  \\\n",
       "0        w         17.0      1.0      121.0      114.0      114.0   \n",
       "1        w         17.0      1.0       96.0       95.0       95.0   \n",
       "2        w         17.0      1.0      130.0      128.0      128.0   \n",
       "3        w         17.0      3.0      104.0      105.0      105.0   \n",
       "4        w         17.0      1.0       98.0       97.0       97.0   \n",
       "\n",
       "                                               text1  \\\n",
       "0  \\n\\n\\n Today, I am going to describe one of my...   \n",
       "1  My niece is 3 years old who is my younger brot...   \n",
       "2  When I was in Germany, I met a friend who was ...   \n",
       "3  my friend is ANON_NAME_0. she is a my ELI frie...   \n",
       "4  Younghun is my best friend. His facial appeara...   \n",
       "\n",
       "  text2 (line breaks/extra spaces removed, spaces added to reach 60)  \\\n",
       "0  Today, I am going to describe one of my classm...                   \n",
       "1  My niece is 3 years old who is my younger brot...                   \n",
       "2  When I was in Germany, I met a friend who was ...                   \n",
       "3  my friend is ANON_NAME_0. she is a my ELI frie...                   \n",
       "4  Younghun is my best friend. His facial appeara...                   \n",
       "\n",
       "               text3 (edits made to fix word counts)  Judgement Notes  \n",
       "0  Today, I am going to describe one of my classm...        1.0   NaN  \n",
       "1  My niece is 3 years old who is my younger brot...        1.0   NaN  \n",
       "2  When I was in Germany, I met a friend who was ...        1.0   NaN  \n",
       "3  my friend is ANON_NAME_0. she is a my ELI frie...        1.0   NaN  \n",
       "4  Younghun is my best friend. His facial appeara...        1.0   NaN  "
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the answer df\n",
    "col='course_id'\n",
    "answer=rmStr(answer_, col) #removes strings from answer_.course_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the datasets\n",
    "\n",
    "merge_ans=answer[['anon_id','course_id']] #sets answer df up for merging \n",
    "student_merge=student.drop(columns=['course_history']) #set student df up for merging\n",
    "\n",
    "student_ans=student_merge.merge(answer,on='anon_id').astype({'course_id':'int64'}) #merges student and answers\n",
    "\n",
    "stu_ans_crs_=student_ans.merge(course_, on='course_id') #merges the student-answers df with course df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>native_language</th>\n",
       "      <th>age</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>course_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>level_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>Some people prefer eat out and some like doing...</td>\n",
       "      <td>['Some', 'people', 'prefer', 'eat', 'out', 'an...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>97</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>288</td>\n",
       "      <td>My opinion is that a person does need educatio...</td>\n",
       "      <td>['My', 'opinion', 'is', 'that', 'a', 'person',...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1957</td>\n",
       "      <td>189</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>There are two national rooms in the Cathedral ...</td>\n",
       "      <td>['There', 'are', 'two', 'national', 'rooms', '...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2164</td>\n",
       "      <td>190</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>464</td>\n",
       "      <td>There are two nation rooms in the Cathedral of...</td>\n",
       "      <td>['There', 'are', 'two', 'nation', 'rooms', 'in...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bv5</td>\n",
       "      <td>Male</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21.0</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>\"Not all learning takes place in the classroom...</td>\n",
       "      <td>['``', 'Not', 'all', 'learning', 'takes', 'pla...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46225</th>\n",
       "      <td>cy7</td>\n",
       "      <td>Female</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47682</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1. emphasis\\n2. appropriate\\n3. requires\\n4. a...</td>\n",
       "      <td>['1', '.', 'emphasis', '2', '.', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46226</th>\n",
       "      <td>fp7</td>\n",
       "      <td>Female</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47823</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.emphasis \\n2.appropriate\\n3.requires\\n4.anal...</td>\n",
       "      <td>['1', '.', 'emphasis', '2', '.', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46227</th>\n",
       "      <td>fq6</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47824</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.emphasis\\n2.appropriate\\n3.requires\\n4.analy...</td>\n",
       "      <td>['1', '.', 'emphasis', '2', '.', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46228</th>\n",
       "      <td>di6</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47787</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1 emphasis\\n2 normal\\n3 requires\\n4 analyze\\n5...</td>\n",
       "      <td>['1', 'emphasis', '2', 'normal', '3', 'require...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46229</th>\n",
       "      <td>fm3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47725</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1- emphasis\\n2- appropriate\\n3- requires\\n4- a...</td>\n",
       "      <td>['1', '-', 'emphasis', '2', '-', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46230 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anon_id   gender native_language   age answer_id question_id  course_id  \\\n",
       "0         do6   Female         Russian  31.0       150           4        117   \n",
       "1         do6   Female         Russian  31.0      1221          97        117   \n",
       "2         do6   Female         Russian  31.0      1957         189        117   \n",
       "3         do6   Female         Russian  31.0      2164         190        117   \n",
       "4         bv5     Male          Arabic  21.0       151           4        117   \n",
       "...       ...      ...             ...   ...       ...         ...        ...   \n",
       "46225     cy7   Female          Korean   NaN     47682        6066       1034   \n",
       "46226     fp7   Female         Turkish   NaN     47823        6066       1034   \n",
       "46227     fq6     Male         Chinese   NaN     47824        6066       1034   \n",
       "46228     di6     Male         Chinese   NaN     47787        6066       1034   \n",
       "46229     fm3  Unknown          Arabic   NaN     47725        6066       1034   \n",
       "\n",
       "      version text_len                                               text  \\\n",
       "0           1      299  Some people prefer eat out and some like doing...   \n",
       "1           1      288  My opinion is that a person does need educatio...   \n",
       "2           1      321  There are two national rooms in the Cathedral ...   \n",
       "3           1      464  There are two nation rooms in the Cathedral of...   \n",
       "4           1      315  \"Not all learning takes place in the classroom...   \n",
       "...       ...      ...                                                ...   \n",
       "46225       1       10  1. emphasis\\n2. appropriate\\n3. requires\\n4. a...   \n",
       "46226       1       10  1.emphasis \\n2.appropriate\\n3.requires\\n4.anal...   \n",
       "46227       1       10  1.emphasis\\n2.appropriate\\n3.requires\\n4.analy...   \n",
       "46228       1       10  1 emphasis\\n2 normal\\n3 requires\\n4 analyze\\n5...   \n",
       "46229       1       10  1- emphasis\\n2- appropriate\\n3- requires\\n4- a...   \n",
       "\n",
       "                                                  tokens  level_id  \n",
       "0      ['Some', 'people', 'prefer', 'eat', 'out', 'an...         5  \n",
       "1      ['My', 'opinion', 'is', 'that', 'a', 'person',...         5  \n",
       "2      ['There', 'are', 'two', 'national', 'rooms', '...         5  \n",
       "3      ['There', 'are', 'two', 'nation', 'rooms', 'in...         5  \n",
       "4      ['``', 'Not', 'all', 'learning', 'takes', 'pla...         5  \n",
       "...                                                  ...       ...  \n",
       "46225  ['1', '.', 'emphasis', '2', '.', 'appropriate'...         3  \n",
       "46226  ['1', '.', 'emphasis', '2', '.', 'appropriate'...         3  \n",
       "46227  ['1', '.', 'emphasis', '2', '.', 'appropriate'...         3  \n",
       "46228  ['1', 'emphasis', '2', 'normal', '3', 'require...         3  \n",
       "46229  ['1', '-', 'emphasis', '2', '-', 'appropriate'...         3  \n",
       "\n",
       "[46230 rows x 12 columns]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans_crs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Answers in Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping all rows where the answer is less than 60 words\n",
    "index=0\n",
    "ind_list=[]\n",
    "for i in stu_ans_crs_.text_len:\n",
    "    txtlen=int(i)\n",
    "    if txtlen < 60:\n",
    "        ind_list.append(index)\n",
    "    index+=1\n",
    "\n",
    "stu_ans_crs_=stu_ans_crs_.drop(labels=ind_list,axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding spaces after punctuation if needed\n",
    "\n",
    "index=0\n",
    "stu_ans_crs=stu_ans_crs_.copy().drop(columns=['gender','index','age','question_id','course_id',\n",
    "                                             'version','tokens'])\n",
    "stu_ans_crs['processed_text']=pd.NaT\n",
    "stu_ans_crs['wordtype_len']=pd.NaT\n",
    "words=[]\n",
    "for text in stu_ans_crs.text:\n",
    "    uwords=[]\n",
    "    text=text.replace('\\n',' ')\n",
    "    nopunc=text.translate(str.maketrans('','',string.punctuation))\n",
    "    nopunc=nopunc.lower()\n",
    "    stu_ans_crs.at[index,'processed_text']=nopunc\n",
    "    words=nopunc.strip().split(\" \")\n",
    "    for word in words:\n",
    "        if word=='':\n",
    "            words.remove(word)\n",
    "        elif word not in uwords:\n",
    "            uwords.append(word)\n",
    "    stu_ans_crs.at[index,'wordtype_len']=len(uwords)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>level_id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>wordtype_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>150</td>\n",
       "      <td>299</td>\n",
       "      <td>Some people prefer eat out and some like doing...</td>\n",
       "      <td>5</td>\n",
       "      <td>some people prefer eat out and some like doing...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1221</td>\n",
       "      <td>288</td>\n",
       "      <td>My opinion is that a person does need educatio...</td>\n",
       "      <td>5</td>\n",
       "      <td>my opinion is that a person does need educatio...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1957</td>\n",
       "      <td>321</td>\n",
       "      <td>There are two national rooms in the Cathedral ...</td>\n",
       "      <td>5</td>\n",
       "      <td>there are two national rooms in the cathedral ...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>2164</td>\n",
       "      <td>464</td>\n",
       "      <td>There are two nation rooms in the Cathedral of...</td>\n",
       "      <td>5</td>\n",
       "      <td>there are two nation rooms in the cathedral of...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bv5</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>151</td>\n",
       "      <td>315</td>\n",
       "      <td>\"Not all learning takes place in the classroom...</td>\n",
       "      <td>5</td>\n",
       "      <td>not all learning takes place in the classroom ...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17252</th>\n",
       "      <td>bh8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>48034</td>\n",
       "      <td>100</td>\n",
       "      <td>I received my Medical Doctor license in Japan ...</td>\n",
       "      <td>3</td>\n",
       "      <td>i received my medical doctor license in japan ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17253</th>\n",
       "      <td>bh8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>48293</td>\n",
       "      <td>138</td>\n",
       "      <td>I introduce my ideal home to you quickly. This...</td>\n",
       "      <td>3</td>\n",
       "      <td>i introduce my ideal home to you quickly this ...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17254</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>47451</td>\n",
       "      <td>101</td>\n",
       "      <td>I will put 5 items in time capsule such as som...</td>\n",
       "      <td>3</td>\n",
       "      <td>i will put 5 items in time capsule such as som...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17255</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>48001</td>\n",
       "      <td>100</td>\n",
       "      <td>I have studied Visual Design since 2002. I gra...</td>\n",
       "      <td>3</td>\n",
       "      <td>i have studied visual design since 2002 i grad...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17256</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>48316</td>\n",
       "      <td>72</td>\n",
       "      <td>I will describe about my ideal home. I want qu...</td>\n",
       "      <td>3</td>\n",
       "      <td>i will describe about my ideal home i want qui...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17257 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anon_id native_language answer_id text_len  \\\n",
       "0         do6         Russian       150      299   \n",
       "1         do6         Russian      1221      288   \n",
       "2         do6         Russian      1957      321   \n",
       "3         do6         Russian      2164      464   \n",
       "4         bv5          Arabic       151      315   \n",
       "...       ...             ...       ...      ...   \n",
       "17252     bh8        Japanese     48034      100   \n",
       "17253     bh8        Japanese     48293      138   \n",
       "17254     cz3          Korean     47451      101   \n",
       "17255     cz3          Korean     48001      100   \n",
       "17256     cz3          Korean     48316       72   \n",
       "\n",
       "                                                    text  level_id  \\\n",
       "0      Some people prefer eat out and some like doing...         5   \n",
       "1      My opinion is that a person does need educatio...         5   \n",
       "2      There are two national rooms in the Cathedral ...         5   \n",
       "3      There are two nation rooms in the Cathedral of...         5   \n",
       "4      \"Not all learning takes place in the classroom...         5   \n",
       "...                                                  ...       ...   \n",
       "17252  I received my Medical Doctor license in Japan ...         3   \n",
       "17253  I introduce my ideal home to you quickly. This...         3   \n",
       "17254  I will put 5 items in time capsule such as som...         3   \n",
       "17255  I have studied Visual Design since 2002. I gra...         3   \n",
       "17256  I will describe about my ideal home. I want qu...         3   \n",
       "\n",
       "                                          processed_text wordtype_len  \n",
       "0      some people prefer eat out and some like doing...          125  \n",
       "1      my opinion is that a person does need educatio...          112  \n",
       "2      there are two national rooms in the cathedral ...          149  \n",
       "3      there are two nation rooms in the cathedral of...          187  \n",
       "4      not all learning takes place in the classroom ...          136  \n",
       "...                                                  ...          ...  \n",
       "17252  i received my medical doctor license in japan ...           54  \n",
       "17253  i introduce my ideal home to you quickly this ...           88  \n",
       "17254  i will put 5 items in time capsule such as som...           67  \n",
       "17255  i have studied visual design since 2002 i grad...           57  \n",
       "17256  i will describe about my ideal home i want qui...           51  \n",
       "\n",
       "[17257 rows x 8 columns]"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Loose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_loose=loose.drop(columns=['3Langs','Item','answer_id','gender','course_id','class_id','question_id','version','text1_len',\n",
    "                                     'text2_len','text1','text2 (line breaks/extra spaces removed, spaces added to reach 60)',\n",
    "                                     'Judgement','Notes'])\n",
    "drop_loose=dropped_loose.dropna()\n",
    "\n",
    "sorted_loose=drop_loose.sort_values(by=['anon_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>level_id</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Barber, chef by profession, but an expert on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>The article \"English as Co star\" support the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>In this article the authors Goleman, Kaufman a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Flow in the sense expressed in the text \"The C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Bill Gates, in this conference, explained two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>Pittsburgh had a big snowstorm recently. A lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Korea, which is connected to China, belongs to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>Koko is a big female gorilla who was born in S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>4.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>A natural disaster is the effect of the natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>I think human language and nonhuman primate la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anon_id       L1  level_id  text3_len  \\\n",
       "0        aa0  Spanish       5.0      109.0   \n",
       "1        aa0  Spanish       5.0      190.0   \n",
       "2        aa0  Spanish       5.0      193.0   \n",
       "3        aa0  Spanish       5.0      170.0   \n",
       "4        aa0  Spanish       5.0       85.0   \n",
       "...      ...      ...       ...        ...   \n",
       "1805     ha2   Korean       5.0      217.0   \n",
       "1806     ha2   Korean       5.0      170.0   \n",
       "1807     ha2   Korean       5.0      195.0   \n",
       "1808     ha2   Korean       4.0      283.0   \n",
       "1809     ha2   Korean       5.0      168.0   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  \n",
       "0     Barber, chef by profession, but an expert on a...  \n",
       "1     The article \"English as Co star\" support the f...  \n",
       "2     In this article the authors Goleman, Kaufman a...  \n",
       "3     Flow in the sense expressed in the text \"The C...  \n",
       "4     Bill Gates, in this conference, explained two ...  \n",
       "...                                                 ...  \n",
       "1805  Pittsburgh had a big snowstorm recently. A lot...  \n",
       "1806  Korea, which is connected to China, belongs to...  \n",
       "1807  Koko is a big female gorilla who was born in S...  \n",
       "1808  A natural disaster is the effect of the natura...  \n",
       "1809  I think human language and nonhuman primate la...  \n",
       "\n",
       "[1810 rows x 5 columns]"
      ]
     },
     "execution_count": 1001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "textcount_loose=sorted_loose.copy()\n",
    "\n",
    "textcount_loose['processed_text']=pd.NaT\n",
    "textcount_loose['wordtype_len']=pd.NaT\n",
    "words=[]\n",
    "for text in textcount_loose['text3 (edits made to fix word counts)']:\n",
    "    uwords=[]\n",
    "    text=text.replace('\\n',' ')\n",
    "    nopunc=text.translate(str.maketrans('','',string.punctuation))\n",
    "    nopunc=nopunc.lower()\n",
    "    textcount_loose.at[index,'processed_text']=nopunc\n",
    "    words=nopunc.strip().split(\" \")\n",
    "    for word in words:\n",
    "        if word=='':\n",
    "            words.remove(word)\n",
    "        elif word not in uwords:\n",
    "            uwords.append(word)\n",
    "    textcount_loose.at[index,'wordtype_len']=len(uwords)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>level_id</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>wordtype_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Barber, chef by profession, but an expert on a...</td>\n",
       "      <td>barber chef by profession but an expert on agr...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>The article \"English as Co star\" support the f...</td>\n",
       "      <td>the article english as co star support the fac...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>In this article the authors Goleman, Kaufman a...</td>\n",
       "      <td>in this article the authors goleman kaufman an...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Flow in the sense expressed in the text \"The C...</td>\n",
       "      <td>flow in the sense expressed in the text the cr...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Bill Gates, in this conference, explained two ...</td>\n",
       "      <td>bill gates in this conference explained two bi...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>Pittsburgh had a big snowstorm recently. A lot...</td>\n",
       "      <td>pittsburgh had a big snowstorm recently a lot ...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Korea, which is connected to China, belongs to...</td>\n",
       "      <td>korea which is connected to china belongs to t...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>Koko is a big female gorilla who was born in S...</td>\n",
       "      <td>koko is a big female gorilla who was born in s...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>4.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>A natural disaster is the effect of the natura...</td>\n",
       "      <td>a natural disaster is the effect of the natura...</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>I think human language and nonhuman primate la...</td>\n",
       "      <td>i think human language and nonhuman primate la...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anon_id       L1  level_id  text3_len  \\\n",
       "0        aa0  Spanish       5.0      109.0   \n",
       "1        aa0  Spanish       5.0      190.0   \n",
       "2        aa0  Spanish       5.0      193.0   \n",
       "3        aa0  Spanish       5.0      170.0   \n",
       "4        aa0  Spanish       5.0       85.0   \n",
       "...      ...      ...       ...        ...   \n",
       "1805     ha2   Korean       5.0      217.0   \n",
       "1806     ha2   Korean       5.0      170.0   \n",
       "1807     ha2   Korean       5.0      195.0   \n",
       "1808     ha2   Korean       4.0      283.0   \n",
       "1809     ha2   Korean       5.0      168.0   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  \\\n",
       "0     Barber, chef by profession, but an expert on a...   \n",
       "1     The article \"English as Co star\" support the f...   \n",
       "2     In this article the authors Goleman, Kaufman a...   \n",
       "3     Flow in the sense expressed in the text \"The C...   \n",
       "4     Bill Gates, in this conference, explained two ...   \n",
       "...                                                 ...   \n",
       "1805  Pittsburgh had a big snowstorm recently. A lot...   \n",
       "1806  Korea, which is connected to China, belongs to...   \n",
       "1807  Koko is a big female gorilla who was born in S...   \n",
       "1808  A natural disaster is the effect of the natura...   \n",
       "1809  I think human language and nonhuman primate la...   \n",
       "\n",
       "                                         processed_text wordtype_len  \n",
       "0     barber chef by profession but an expert on agr...           79  \n",
       "1     the article english as co star support the fac...          117  \n",
       "2     in this article the authors goleman kaufman an...          122  \n",
       "3     flow in the sense expressed in the text the cr...          103  \n",
       "4     bill gates in this conference explained two bi...           66  \n",
       "...                                                 ...          ...  \n",
       "1805  pittsburgh had a big snowstorm recently a lot ...          129  \n",
       "1806  korea which is connected to china belongs to t...          101  \n",
       "1807  koko is a big female gorilla who was born in s...          115  \n",
       "1808  a natural disaster is the effect of the natura...          135  \n",
       "1809  i think human language and nonhuman primate la...          100  \n",
       "\n",
       "[1810 rows x 7 columns]"
      ]
     },
     "execution_count": 1003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcount_loose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Ids of participants in all three of Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_lvls=stu_ans_crs[['anon_id','native_language','level_id']]\n",
    "sac_lvls_rkc=sac_lvls\n",
    "\n",
    "index=0\n",
    "for i in sac_lvls_rkc.native_language:\n",
    "    if (i != 'Korean' and i != 'Spanish' and i != 'Chinese'):\n",
    "        sac_lvls_rkc=sac_lvls_rkc.drop(index)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>level_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17243</th>\n",
       "      <td>ew6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17244</th>\n",
       "      <td>ew6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17254</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17255</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17256</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7629 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anon_id native_language  level_id\n",
       "13        ax4         Chinese         5\n",
       "14        ax4         Chinese         5\n",
       "15        ax4         Chinese         5\n",
       "16        ax4         Chinese         5\n",
       "17        ax4         Chinese         5\n",
       "...       ...             ...       ...\n",
       "17243     ew6         Chinese         3\n",
       "17244     ew6         Chinese         3\n",
       "17254     cz3          Korean         3\n",
       "17255     cz3          Korean         3\n",
       "17256     cz3          Korean         3\n",
       "\n",
       "[7629 rows x 3 columns]"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac_lvls_rkc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to see what levels each anon_id participated in \n",
    "lvl_list=sac_lvls_rkc.groupby('anon_id').agg(n_uniq=('level_id','nunique'), lvl_nums=('level_id',get_uniques))\n",
    "lvl_list=lvl_list.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>n_uniq</th>\n",
       "      <th>lvl_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa1</td>\n",
       "      <td>1</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa8</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa9</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>ha0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ha2</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>ha6</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>hb4</td>\n",
       "      <td>3</td>\n",
       "      <td>[5, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>hc1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id  n_uniq   lvl_nums\n",
       "0       aa0       1        [5]\n",
       "1       aa1       1        [4]\n",
       "2       aa3       1        [4]\n",
       "3       aa8       2     [5, 4]\n",
       "4       aa9       3  [3, 4, 2]\n",
       "..      ...     ...        ...\n",
       "471     ha0       1        [5]\n",
       "472     ha2       2     [5, 4]\n",
       "473     ha6       2     [3, 4]\n",
       "474     hb4       3  [5, 4, 3]\n",
       "475     hc1       1        [5]\n",
       "\n",
       "[476 rows x 3 columns]"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will create a list of only the ids that have levels 3, 4, and 5 in their lvl_nums column\n",
    "all_three=[]\n",
    "ind=0\n",
    "for i in lvl_list.anon_id:\n",
    "    if ((3 in lvl_list.iat[ind,2]) and (4 in lvl_list.iat[ind,2]) and (5 in lvl_list.iat[ind,2])):\n",
    "        append=lvl_list.at[ind, 'anon_id']\n",
    "        all_three.append(append)\n",
    "    ind+=1\n",
    "\n",
    "all_three.sort()\n",
    "len(all_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anon=[]\n",
    "for i in sac_lvls_rkc.anon_id:\n",
    "    all_anon.append(i)\n",
    "len(all_anon)\n",
    "all_anon.sort()\n",
    "\n",
    "sorted_sac=sac_lvls_rkc.sort_values(by=['anon_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['ag9', 'an5', 'as0', 'ay1', 'be7', 'bv9', 'bz5', 'cc4', 'cj8', 'co5', 'cv3', 'cw0', 'ea4', 'eq8', 'es9', 'fj7', 'fp5', 'fu6', 'gc2', 'gq8', 'gz2', 'hb4'], 'Chinese': ['aq1', 'ar8', 'bf7', 'bl4', 'bl7', 'bp4', 'bz2', 'cb3', 'cf9', 'cz4', 'dk6', 'ev9', 'fi1', 'fj4', 'gb4', 'gx5'], 'Spanish': ['bj2', 'cm9', 'fa2', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "lang_dict_all={}\n",
    "for i in all_three:\n",
    "    df_ind=binarySearchArr(all_anon,i)\n",
    "    key=sorted_sac.at[df_ind,'native_language']\n",
    "    val=sorted_sac.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in lang_dict_all:\n",
    "        lang_dict_all[key].append(val)\n",
    "    else:\n",
    "        lang_dict_all[key]=[val]\n",
    "    \n",
    "print(lang_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will create a list of only the ids that have levels 3 and 4 in their lvl_nums column\n",
    "lvls34=[]\n",
    "ind=0\n",
    "for i in lvl_list.anon_id:\n",
    "    if ((3 in lvl_list.iat[ind,2]) and (4 in lvl_list.iat[ind,2])):\n",
    "        append=lvl_list.at[ind, 'anon_id']\n",
    "        lvls34.append(append)\n",
    "    ind+=1\n",
    "\n",
    "lvls34.sort()\n",
    "len(lvls34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa9', 'af1', 'ag9', 'ah9', 'an4', 'an5', 'as0', 'as4', 'ay1', 'bc0', 'be2', 'be7', 'bf1', 'bm0', 'br5', 'bv9', 'bz5', 'ca6', 'cc4', 'cf6', 'ch9', 'cj8', 'cn3', 'co5', 'cv3', 'cw0', 'cw6', 'cy5', 'dd1', 'dj6', 'dp5', 'ea4', 'ef4', 'eo2', 'eq8', 'es9', 'et1', 'ex0', 'fc9', 'fh7', 'fi4', 'fj7', 'fk1', 'fl4', 'fl5', 'fo3', 'fp5', 'fp9', 'fu6', 'fv7', 'fv9', 'gb7', 'gc2', 'gd1', 'gl4', 'gq8', 'gv1', 'gz2', 'ha6', 'hb4'], 'Chinese': ['ag6', 'ai4', 'ap5', 'ap8', 'aq1', 'ar8', 'ax7', 'ba3', 'bf7', 'bl4', 'bl7', 'bp4', 'bu9', 'bz2', 'cb3', 'cf9', 'cl2', 'cl6', 'cq2', 'cs5', 'cz4', 'dk6', 'dl8', 'dm4', 'do7', 'dp1', 'dt4', 'dx1', 'eb6', 'ev5', 'ev9', 'ey8', 'ff6', 'fi1', 'fj4', 'fm2', 'fn8', 'fy3', 'gb4', 'gw5', 'gw8', 'gx5'], 'Spanish': ['bj2', 'ch0', 'cm9', 'en3', 'fa2', 'fe7', 'fg7', 'fy1', 'fy6']}\n"
     ]
    }
   ],
   "source": [
    "lang_dict_34={}\n",
    "for i in lvls34:\n",
    "    df_ind=binarySearchArr(all_anon,i)\n",
    "    key=sorted_sac.at[df_ind,'native_language']\n",
    "    val=sorted_sac.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in lang_dict_34:\n",
    "        lang_dict_34[key].append(val)\n",
    "    else:\n",
    "        lang_dict_34[key]=[val]\n",
    "    \n",
    "print(lang_dict_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will create a list of only the ids that have levels 3 and 4 in their lvl_nums column\n",
    "lvls45=[]\n",
    "ind=0\n",
    "for i in lvl_list.anon_id:\n",
    "    if ((4 in lvl_list.iat[ind,2]) and (5 in lvl_list.iat[ind,2])):\n",
    "        append=lvl_list.at[ind, 'anon_id']\n",
    "        lvls45.append(append)\n",
    "    ind+=1\n",
    "\n",
    "lvls45.sort()\n",
    "len(lvls45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa8', 'ac5', 'ad1', 'ag9', 'al5', 'an5', 'ap4', 'aq5', 'as0', 'as7', 'au5', 'au6', 'aw6', 'ay1', 'be7', 'bq0', 'bu4', 'bv9', 'bw3', 'bz5', 'cc4', 'ce3', 'ch2', 'ci0', 'cj8', 'co5', 'cp0', 'cv3', 'cw0', 'cw1', 'dd9', 'df3', 'dm3', 'dy7', 'ea4', 'eb3', 'eb9', 'ec1', 'eg5', 'eq8', 'es0', 'es9', 'et3', 'ex3', 'fa0', 'fb4', 'fd6', 'ff1', 'fi5', 'fj7', 'fl0', 'fp5', 'fs0', 'ft2', 'fu6', 'fx4', 'fz8', 'ga1', 'gc2', 'ge6', 'gg2', 'gg6', 'gj0', 'gk5', 'gl1', 'gn0', 'gq8', 'gs3', 'gu0', 'gz2', 'ha2', 'hb4'], 'Chinese': ['ad7', 'ae9', 'af4', 'am5', 'an7', 'aq1', 'ar8', 'ar9', 'az8', 'bd7', 'bf2', 'bf7', 'bf9', 'bl4', 'bl7', 'bp2', 'bp4', 'br9', 'bv8', 'bw9', 'by5', 'by6', 'bz1', 'bz2', 'ca4', 'cb3', 'cd6', 'cf9', 'ci2', 'cj5', 'cl3', 'cz2', 'cz4', 'dc0', 'di7', 'dk6', 'dm8', 'dq9', 'du9', 'dw2', 'ei2', 'ei8', 'eo1', 'eq4', 'es4', 'et4', 'ev6', 'ev9', 'ff2', 'fg8', 'fi1', 'fj4', 'fk8', 'fo4', 'fq5', 'fw7', 'ga3', 'gb4', 'gf5', 'gl2', 'gm1', 'go8', 'gt0', 'gv3', 'gw0', 'gx5'], 'Spanish': ['bi4', 'bj2', 'br2', 'cm9', 'cv7', 'dc6', 'df4', 'dk0', 'eo8', 'fa2', 'fx7', 'fy1', 'gq4', 'gs6', 'gu1', 'gu9']}\n"
     ]
    }
   ],
   "source": [
    "lang_dict_45={}\n",
    "for i in lvls45:\n",
    "    df_ind=binarySearchArr(all_anon,i)\n",
    "    key=sorted_sac.at[df_ind,'native_language']\n",
    "    val=sorted_sac.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in lang_dict_45:\n",
    "        lang_dict_45[key].append(val)\n",
    "    else:\n",
    "        lang_dict_45[key]=[val]\n",
    "    \n",
    "print(lang_dict_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-5:  42 \n",
      "3-4:  111 \n",
      "4-5:  154\n"
     ]
    }
   ],
   "source": [
    "print(\"3-5: \",len(all_three),'\\n3-4: ',len(lvls34),\"\\n4-5: \",len(lvls45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Three:\n",
      "Korean 22\n",
      "Chinese 16\n",
      "Spanish 4\n"
     ]
    }
   ],
   "source": [
    "print(\"All Three:\")\n",
    "for key, value in lang_dict_all.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-4:\n",
      "Korean 60\n",
      "Chinese 42\n",
      "Spanish 9\n"
     ]
    }
   ],
   "source": [
    "print(\"3-4:\")\n",
    "\n",
    "for key, value in lang_dict_34.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-5:\n",
      "Korean 72\n",
      "Chinese 66\n",
      "Spanish 16\n"
     ]
    }
   ],
   "source": [
    "print(\"4-5:\")\n",
    "\n",
    "for key, value in lang_dict_45.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting distributions of Loose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1810"
      ]
     },
     "execution_count": 1004,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the columns that aren't needed for what we're doing right now\n",
    "\n",
    "all_anon_loose=[]\n",
    "for i in sorted_loose.anon_id:\n",
    "    all_anon_loose.append(i)\n",
    "len(all_anon_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>n_anon_ids_loose</th>\n",
       "      <th>lvl_nums_loose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa8</td>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa9</td>\n",
       "      <td>2</td>\n",
       "      <td>[3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ab6</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>gz2</td>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>gz5</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>gz7</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ha0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>ha2</td>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id  n_anon_ids_loose lvl_nums_loose\n",
       "0       aa0                 1          [5.0]\n",
       "1       aa3                 1          [4.0]\n",
       "2       aa8                 2     [4.0, 5.0]\n",
       "3       aa9                 2     [3.0, 4.0]\n",
       "4       ab6                 1          [4.0]\n",
       "..      ...               ...            ...\n",
       "278     gz2                 2     [4.0, 5.0]\n",
       "279     gz5                 1          [4.0]\n",
       "280     gz7                 1          [4.0]\n",
       "281     ha0                 1          [5.0]\n",
       "282     ha2                 2     [4.0, 5.0]\n",
       "\n",
       "[283 rows x 3 columns]"
      ]
     },
     "execution_count": 1005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code gets how many times an id appeared (n_anon_ids) as well as an array of what levels they participated in (lvl_nums)\n",
    "clean_ids_lvl_loose=drop_loose.groupby('anon_id').agg(\n",
    "    n_anon_ids_loose=('level_id', 'nunique'),\n",
    "    lvl_nums_loose=('level_id', get_uniques)\n",
    ")\n",
    "\n",
    "#changes the clean_ids_lvl's index so we can access the anon_id easier\n",
    "ind_loose= clean_ids_lvl_loose.reset_index()\n",
    "ind_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1006,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for loop that checks if lvl_nums has all three levels in it (3,4, and 5)\n",
    "# if there are three values in lvl_nums, the anon_id is appended to the all_three list\n",
    "ind=0\n",
    "all_three_loose=[]\n",
    "append=''\n",
    "for i in ind_loose.anon_id:\n",
    "    if ind_loose.iat[ind, 1]==3:\n",
    "        append=ind_loose.at[ind,'anon_id']\n",
    "        all_three_loose.append(append)\n",
    "    ind=ind+1\n",
    "\n",
    "all_three_loose.sort()\n",
    "len(all_three_loose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# creates list of participants in loose sheet that are in level 3 and 4\n",
    "ind=0\n",
    "in34_loose=[]\n",
    "append=''\n",
    "for i in ind_loose.anon_id:\n",
    "    if ind_loose.iat[ind, 1]==2:\n",
    "        if (3 in ind_loose.iat[ind,2] and 4 in ind_loose.iat[ind,2]):\n",
    "            append=ind_loose.at[ind,'anon_id']\n",
    "            in34_loose.append(append)\n",
    "    elif ind_loose.iat[ind,1]==3:\n",
    "        append=ind_loose.at[ind,'anon_id']\n",
    "        in34_loose.append(append)\n",
    "    ind=ind+1\n",
    "in34_loose.sort()\n",
    "print(len(in34_loose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['an5', 'bv9'], 'Chinese': ['ar8', 'bl4', 'bp4', 'bz2'], 'Spanish': ['fa2', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "llang_dict_35={}\n",
    "for i in all_three_loose:\n",
    "    df_ind=binarySearchArr(all_anon_loose,i)\n",
    "    key=sorted_loose.at[df_ind,'L1']\n",
    "    val=sorted_loose.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in llang_dict_35:\n",
    "        llang_dict_35[key].append(val)\n",
    "    else:\n",
    "        llang_dict_35[key]=[val]\n",
    "    \n",
    "print(llang_dict_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa9', 'ah9', 'an5', 'be2', 'bf1', 'bv9', 'cf6', 'cw6', 'cy5', 'ex0', 'fc9', 'fv7', 'gd1', 'gv1'], 'Chinese': ['ap5', 'ar8', 'ax7', 'bl4', 'bp4', 'bz2', 'cs5', 'ey8', 'fn8'], 'Spanish': ['ch0', 'cm9', 'fa2', 'fe7', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "llang_dict_34={}\n",
    "for i in in34_loose:\n",
    "    df_ind=binarySearchArr(all_anon_loose,i)\n",
    "    key=sorted_loose.at[df_ind,'L1']\n",
    "    val=sorted_loose.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in llang_dict_34:\n",
    "        llang_dict_34[key].append(val)\n",
    "    else:\n",
    "        llang_dict_34[key]=[val]\n",
    "    \n",
    "print(llang_dict_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "# creates list of participants in loose sheet that are in level 4 and 5\n",
    "ind=0\n",
    "in45_loose=[]\n",
    "append=''\n",
    "for i in ind_loose.anon_id:\n",
    "    if ind_loose.iat[ind, 1]==2:\n",
    "        if (4 in ind_loose.iat[ind,2] and 5 in ind_loose.iat[ind,2]):\n",
    "            append=ind_loose.at[ind,'anon_id']\n",
    "            in45_loose.append(append)\n",
    "    elif ind_loose.iat[ind,1]==3:\n",
    "        append=ind_loose.at[ind,'anon_id']\n",
    "        in45_loose.append(append)\n",
    "    ind=ind+1\n",
    "in45_loose.sort()\n",
    "print(len(in45_loose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa8', 'ac5', 'al5', 'an5', 'au5', 'au6', 'bu4', 'bv9', 'cc4', 'ce3', 'cj8', 'co5', 'cw1', 'dd9', 'dy7', 'fa0', 'ff1', 'fj7', 'fp5', 'fu6', 'fx4', 'gq8', 'gz2', 'ha2'], 'Chinese': ['aq1', 'ar8', 'ar9', 'az8', 'bf2', 'bl4', 'bl7', 'bp4', 'bz2', 'cb3', 'cj5', 'cz4', 'dc0', 'dm8', 'dq9', 'ei8', 'et4', 'ev6', 'ev9', 'ff2', 'fg8', 'fk8', 'gb4', 'gf5', 'gm1'], 'Spanish': ['bi4', 'bj2', 'cv7', 'dc6', 'df4', 'dk0', 'eo8', 'fa2', 'fx7', 'fy1', 'gq4', 'gu1']}\n"
     ]
    }
   ],
   "source": [
    "llang_dict_45={}\n",
    "for i in in45_loose:\n",
    "    df_ind=binarySearchArr(all_anon_loose,i)\n",
    "    key=sorted_loose.at[df_ind,'L1']\n",
    "    val=sorted_loose.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in llang_dict_45:\n",
    "        llang_dict_45[key].append(val)\n",
    "    else:\n",
    "        llang_dict_45[key]=[val]\n",
    "    \n",
    "print(llang_dict_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-5:\n",
      "Korean 2\n",
      "Chinese 4\n",
      "Spanish 2\n"
     ]
    }
   ],
   "source": [
    "print(\"3-5:\")\n",
    "\n",
    "for key, value in llang_dict_35.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-4:\n",
      "Korean 14\n",
      "Chinese 9\n",
      "Spanish 5\n"
     ]
    }
   ],
   "source": [
    "print(\"3-4:\")\n",
    "\n",
    "for key, value in llang_dict_34.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-5:\n",
      "Korean 24\n",
      "Chinese 25\n",
      "Spanish 12\n"
     ]
    }
   ],
   "source": [
    "print(\"4-5:\")\n",
    "\n",
    "for key, value in llang_dict_45.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer counts for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_counts=sorted_sac.copy()\n",
    "e_counts[\"Number of Questions\"]=e_counts.anon_id.count()\n",
    "entire_counts=e_counts.groupby(['anon_id','native_language'])['Number of Questions'].count()\n",
    "qcount_entire=entire_counts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>Number of Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa1</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa3</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>ha0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>ha6</td>\n",
       "      <td>Korean</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>hb4</td>\n",
       "      <td>Korean</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>hc1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id native_language  Number of Questions\n",
       "0       aa0         Spanish                   17\n",
       "1       aa1         Chinese                   22\n",
       "2       aa3         Chinese                    8\n",
       "3       aa8          Korean                   34\n",
       "4       aa9          Korean                   28\n",
       "..      ...             ...                  ...\n",
       "471     ha0          Korean                    3\n",
       "472     ha2          Korean                   44\n",
       "473     ha6          Korean                   32\n",
       "474     hb4          Korean                   39\n",
       "475     hc1          Korean                    5\n",
       "\n",
       "[476 rows x 3 columns]"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qcount_entire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in all 3\n",
      "Spanish counts:  107 \n",
      "Chinese counts:  468 \n",
      "Korean counts:  795\n"
     ]
    }
   ],
   "source": [
    "espanish,echinese,ekorean=find_counts(qcount_entire,all_three,'native_language')\n",
    "print('Counts in all 3\\nSpanish counts: ',espanish,'\\nChinese counts: ',echinese,'\\nKorean counts: ',ekorean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 3 and 4\n",
      "Spanish counts:  156 \n",
      "Chinese counts:  928 \n",
      "Korean counts:  1564\n"
     ]
    }
   ],
   "source": [
    "espanish34,echinese34,ekorean34=find_counts(qcount_entire,lvls34,'native_language')\n",
    "print('Counts in 3 and 4\\nSpanish counts: ',espanish34,'\\nChinese counts: ',echinese34,'\\nKorean counts: ',ekorean34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 4 and 5\n",
      "Spanish counts:  298 \n",
      "Chinese counts:  1712 \n",
      "Korean counts:  1975\n"
     ]
    }
   ],
   "source": [
    "espanish45,echinese45,ekorean45=find_counts(qcount_entire,lvls45,'native_language')\n",
    "print('Counts in 4 and 5\\nSpanish counts: ',espanish45,'\\nChinese counts: ',echinese45,'\\nKorean counts: ',ekorean45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Counts for Loose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>Number of Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa3</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ab6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>gz2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>gz5</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>gz7</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ha0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id       L1  Number of Questions\n",
       "0       aa0  Spanish                    8\n",
       "1       aa3  Chinese                    2\n",
       "2       aa8   Korean                   13\n",
       "3       aa9   Korean                   18\n",
       "4       ab6  Chinese                    2\n",
       "..      ...      ...                  ...\n",
       "278     gz2   Korean                    8\n",
       "279     gz5  Chinese                    6\n",
       "280     gz7   Korean                    5\n",
       "281     ha0   Korean                    3\n",
       "282     ha2   Korean                   19\n",
       "\n",
       "[283 rows x 3 columns]"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_counts=final_loose.copy()\n",
    "l_counts[\"Number of Questions\"]=l_counts.anon_id.count()\n",
    "loose_counts=l_counts.groupby(['anon_id','L1'])['Number of Questions'].count()\n",
    "qcount_loose=loose_counts.reset_index()\n",
    "qcount_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in all three\n",
      "Spanish counts:  23 \n",
      "Chinese counts:  34 \n",
      "Korean counts:  26\n"
     ]
    }
   ],
   "source": [
    "lspanish,lchinese,lkorean=find_counts(qcount_loose, all_three_loose,'L1')\n",
    "print('Counts in all three\\nSpanish counts: ',lspanish,'\\nChinese counts: ',lchinese,'\\nKorean counts: ',lkorean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 3 and 4\n",
      "Spanish counts:  44 \n",
      "Chinese counts:  64 \n",
      "Korean counts:  146\n"
     ]
    }
   ],
   "source": [
    "lspanish34,lchinese34,lkorean34=find_counts(qcount_loose,in34_loose,'L1')\n",
    "print('Counts in 3 and 4\\nSpanish counts: ',lspanish34,'\\nChinese counts: ',lchinese34,'\\nKorean counts: ',lkorean34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 4 and 5\n",
      "Spanish counts:  128 \n",
      "Chinese counts:  305 \n",
      "Korean counts:  289\n"
     ]
    }
   ],
   "source": [
    "lspanish45,lchinese45,lkorean45=find_counts(qcount_loose, in45_loose,'L1')\n",
    "print('Counts in 4 and 5\\nSpanish counts: ',lspanish45,'\\nChinese counts: ',lchinese45,'\\nKorean counts: ',lkorean45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokinizing Essays for Loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>level_id</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bu4</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>\"All I ask of you~~\" With sweet melodies, my e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cz4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>\"Flow\" is that you do something you like to do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fs4</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>4.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>\"Friendship is a plant which we must often wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gs6</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>\"Get Smart\" is a movie directed by Peter Segal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cu2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>\"Human beings are social by their nature\" Afla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>ck9</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>they conduct NEO-PI-R survey which can measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>bv9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>when I am in deserted tropical island, anyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>bl4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>when I was little, I always go home late after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>ay1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>when i was studied in college. i was spent too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>ch0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>your old friend is who made you think about yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anon_id       L1  level_id  text3_len  \\\n",
       "0        bu4   Korean       5.0      849.0   \n",
       "1        cz4  Chinese       5.0      175.0   \n",
       "2        fs4  Spanish       4.0      235.0   \n",
       "3        gs6  Spanish       5.0      190.0   \n",
       "4        cu2  Spanish       5.0      292.0   \n",
       "...      ...      ...       ...        ...   \n",
       "1805     ck9  Chinese       5.0       92.0   \n",
       "1806     bv9   Korean       3.0      157.0   \n",
       "1807     bl4  Chinese       5.0       60.0   \n",
       "1808     ay1   Korean       5.0      142.0   \n",
       "1809     ch0  Spanish       4.0       62.0   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  \n",
       "0     \"All I ask of you~~\" With sweet melodies, my e...  \n",
       "1     \"Flow\" is that you do something you like to do...  \n",
       "2     \"Friendship is a plant which we must often wat...  \n",
       "3     \"Get Smart\" is a movie directed by Peter Segal...  \n",
       "4     \"Human beings are social by their nature\" Afla...  \n",
       "...                                                 ...  \n",
       "1805  they conduct NEO-PI-R survey which can measure...  \n",
       "1806  when I am in deserted tropical island, anyone ...  \n",
       "1807  when I was little, I always go home late after...  \n",
       "1808  when i was studied in college. i was spent too...  \n",
       "1809  your old friend is who made you think about yo...  \n",
       "\n",
       "[1810 rows x 5 columns]"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsort_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three\n",
      "Spanish Token Average:  132.8695652173913 \n",
      "Korean Token Average:  181.3846153846154 \n",
      "Chinese Token Average: 188.64705882352942\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three')\n",
    "finding_tokens(loose,all_three_loose,'L1','text3_len',qcount_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4\n",
      "Spanish Token Average:  140.8181818181818 \n",
      "Korean Token Average:  152.14383561643837 \n",
      "Chinese Token Average: 169.953125\n"
     ]
    }
   ],
   "source": [
    "print('Loose 3-4')\n",
    "finding_tokens(loose,in34_loose,'L1','text3_len',qcount_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5\n",
      "Spanish Token Average:  203.5 \n",
      "Korean Token Average:  231.42560553633217 \n",
      "Chinese Token Average: 238.32131147540983\n"
     ]
    }
   ],
   "source": [
    "print('Loose 4-5')\n",
    "finding_tokens(loose,in45_loose,'L1','text3_len',qcount_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three, Level 3\n",
      "Spanish Average:  101.8\n",
      "Korean Average:  134.5\n",
      "Chinese Average:  131.8181818181818\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three, Level 3')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',all_three_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three, Level 4\n",
      "Spanish Average:  172.5\n",
      "Korean Average:  191.5\n",
      "Chinese Average:  264.14285714285717\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three, Level 4')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',all_three_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three, Level 5\n",
      "Spanish Average:  104.33333333333333\n",
      "Korean Average:  211.25\n",
      "Chinese Average:  194.6875\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three, Level 5')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',all_three_loose,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 3\n",
      "Spanish Average:  108.8\n",
      "Korean Average:  121.32142857142857\n",
      "Chinese Average:  138.33333333333334\n"
     ]
    }
   ],
   "source": [
    "print('Loose 3-4, Level 3')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in34_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 4\n",
      "Spanish Average:  176.52380952380952\n",
      "Korean Average:  169.46511627906978\n",
      "Chinese Average:  200.66666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Loose 3-4, Level 4')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in34_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 4\n",
      "Spanish Average:  214.77272727272728\n",
      "Korean Average:  186.7877094972067\n",
      "Chinese Average:  208.28947368421052\n"
     ]
    }
   ],
   "source": [
    "print('Loose 4-5, Level 4')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in45_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 5\n",
      "Spanish Average:  208.75\n",
      "Korean Average:  313.84615384615387\n",
      "Chinese Average:  278.71830985915494\n"
     ]
    }
   ],
   "source": [
    "print('Loose 4-5, Level 5')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in45_loose,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Essays for Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_sac=stu_ans_crs.sort_values(by=['anon_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three\n",
      "Spanish Token Average:  184.52336448598132 \n",
      "Korean Token Average:  211.71446540880504 \n",
      "Chinese Token Average: 210.15811965811966\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three')\n",
    "finding_tokens(stu_ans_crs,all_three,'native_language','text_len',qcount_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4\n",
      "Spanish Token Average:  189.34615384615384 \n",
      "Korean Token Average:  188.193094629156 \n",
      "Chinese Token Average: 193.5625\n"
     ]
    }
   ],
   "source": [
    "print('Whole 3-4')\n",
    "finding_tokens(stu_ans_crs,lvls34,'native_language','text_len',qcount_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5\n",
      "Spanish Token Average:  220.26174496644296 \n",
      "Korean Token Average:  238.4592405063291 \n",
      "Chinese Token Average: 247.60105140186917\n"
     ]
    }
   ],
   "source": [
    "print('Whole 4-5')\n",
    "finding_tokens(stu_ans_crs,lvls45,'native_language','text_len',qcount_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three, Level 3\n",
      "Spanish Average:  107.88888888888889\n",
      "Korean Average:  165.60355029585799\n",
      "Chinese Average:  141.64655172413794\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three, Level 3')\n",
    "tokenAvg(sort_sac,'text_len','native_language',all_three,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three, Level 4\n",
      "Spanish Average:  252.86274509803923\n",
      "Korean Average:  235.44985673352434\n",
      "Chinese Average:  207.0157894736842\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three, Level 4')\n",
    "tokenAvg(sort_sac,'text_len','native_language',all_three,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three, Level 5\n",
      "Spanish Average:  148.2\n",
      "Korean Average:  209.942238267148\n",
      "Chinese Average:  266.6477987421384\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three, Level 5')\n",
    "tokenAvg(sort_sac,'text_len','native_language',all_three,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 3\n",
      "Spanish Average:  118.28787878787878\n",
      "Korean Average:  142.38\n",
      "Chinese Average:  142.69552238805971\n"
     ]
    }
   ],
   "source": [
    "print('Whole 3-4, Level 3')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls34,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 4\n",
      "Spanish Average:  268.1\n",
      "Korean Average:  212.6007802340702\n",
      "Chinese Average:  207.03712296983758\n"
     ]
    }
   ],
   "source": [
    "print('Whole 3-4, Level 4')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls34,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 4\n",
      "Spanish Average:  243.87412587412587\n",
      "Korean Average:  226.4093686354379\n",
      "Chinese Average:  227.32906530089627\n"
     ]
    }
   ],
   "source": [
    "print('Whole 4-5, Level 4')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls45,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 5\n",
      "Spanish Average:  225.88235294117646\n",
      "Korean Average:  267.7621359223301\n",
      "Chinese Average:  282.9125615763547\n"
     ]
    }
   ],
   "source": [
    "print('Whole 4-5, Level 5')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls45,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Unique Word Counts for Loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5, Level 3\n",
      "Spanish Average:  59.6\n",
      "Korean Average:  74.16666666666667\n",
      "Chinese Average:  72.27272727272727\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5, Level 3\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',all_three_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5, Level 4\n",
      "Spanish Average:  92.7\n",
      "Korean Average:  96.875\n",
      "Chinese Average:  122.14285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5, Level 4\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',all_three_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5, Level 5\n",
      "Spanish Average:  67.66666666666667\n",
      "Korean Average:  116.5\n",
      "Chinese Average:  99.4375\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5, Level 5\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',all_three_loose,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 3\n",
      "Spanish Average:  62.5\n",
      "Korean Average:  69.05357142857143\n",
      "Chinese Average:  73.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4, Level 3\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in34_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 4\n",
      "Spanish Average:  92.71428571428571\n",
      "Korean Average:  90.70930232558139\n",
      "Chinese Average:  102.27777777777777\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4, Level 4\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in34_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 4\n",
      "Spanish Average:  110.63636363636364\n",
      "Korean Average:  98.26815642458101\n",
      "Chinese Average:  104.84210526315789\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5, Level 4\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in45_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 5\n",
      "Spanish Average:  110.61538461538461\n",
      "Korean Average:  144.01923076923077\n",
      "Chinese Average:  133.6549295774648\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5, Level 5\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in45_loose,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Word Counts Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 3\n",
      "Spanish Average:  61.19444444444444\n",
      "Korean Average:  88.14792899408285\n",
      "Chinese Average:  79.16379310344827\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 3\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',all_three,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 4\n",
      "Spanish Average:  119.09803921568627\n",
      "Korean Average:  114.41833810888252\n",
      "Chinese Average:  104.41052631578947\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 4\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',all_three,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 5\n",
      "Spanish Average:  85.6\n",
      "Korean Average:  102.11191335740072\n",
      "Chinese Average:  126.59119496855345\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 5\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',all_three,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 3\n",
      "Spanish Average:  65.24242424242425\n",
      "Korean Average:  78.616\n",
      "Chinese Average:  76.10149253731343\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 3\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls34,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 4\n",
      "Spanish Average:  123.15714285714286\n",
      "Korean Average:  105.76462938881664\n",
      "Chinese Average:  103.89095127610209\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 4\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls34,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 4\n",
      "Spanish Average:  119.0909090909091\n",
      "Korean Average:  111.42871690427698\n",
      "Chinese Average:  114.30857874519846\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 4\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls45,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 5\n",
      "Spanish Average:  114.26890756302521\n",
      "Korean Average:  124.23300970873787\n",
      "Chinese Average:  132.72783251231527\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 5\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls45,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token/Type Ratio Loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5 Ratio, Level 3\n",
      "Spanish Ratio:  0.5854616895874264\n",
      "Korean Ratio:  0.5514250309789344\n",
      "Chinese Ratio:  0.5482758620689655\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5 Ratio, Level 3\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',all_three_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5 Ratio, Level 4\n",
      "Spanish Ratio:  0.5373913043478261\n",
      "Korean Ratio:  0.5058746736292428\n",
      "Chinese Ratio:  0.46241211465657106\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5 Ratio, Level 4\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',all_three_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5 Ratio, Level 5\n",
      "Spanish Ratio:  0.6485623003194889\n",
      "Korean Ratio:  0.5514792899408284\n",
      "Chinese Ratio:  0.5107544141252006\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5 Ratio, Level 5\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',all_three_loose,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4 Ratio, Level 3\n",
      "Spanish Ratio:  0.5744485294117647\n",
      "Korean Ratio:  0.5691786870768325\n",
      "Chinese Ratio:  0.528433734939759\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4 Ratio, Level 3\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in34_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4 Ratio, Level 4\n",
      "Spanish Ratio:  0.5252225519287833\n",
      "Korean Ratio:  0.535268285988747\n",
      "Chinese Ratio:  0.5096899224806202\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4 Ratio, Level 4\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in34_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5 Ratio, Level 4\n",
      "Spanish Ratio:  0.5151322751322751\n",
      "Korean Ratio:  0.5260954090025423\n",
      "Chinese Ratio:  0.503348073278585\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5 Ratio, Level 4\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in45_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5 Ratio, Level 5\n",
      "Spanish Ratio:  0.5298940580377706\n",
      "Korean Ratio:  0.4588848039215686\n",
      "Chinese Ratio:  0.47953408459245034\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5 Ratio, Level 5\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in45_loose,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type/Token Ratio Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 3\n",
      "Spanish Ratio:  0.5671987641606591\n",
      "Korean Ratio:  0.532282845606889\n",
      "Chinese Ratio:  0.5588825999634836\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 3\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',all_three,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 4\n",
      "Spanish Ratio:  0.4709987593052109\n",
      "Korean Ratio:  0.4859562868130264\n",
      "Chinese Ratio:  0.5043602064424274\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 4\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',all_three,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 5\n",
      "Spanish Ratio:  0.5775978407557355\n",
      "Korean Ratio:  0.4863809884100836\n",
      "Chinese Ratio:  0.4747505719744321\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 5\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',all_three,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 3\n",
      "Spanish Ratio:  0.551556295632125\n",
      "Korean Ratio:  0.5521562017137238\n",
      "Chinese Ratio:  0.5333138087567726\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 3\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls34,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 4\n",
      "Spanish Ratio:  0.45937017104491923\n",
      "Korean Ratio:  0.4974799681937733\n",
      "Chinese Ratio:  0.5017986619300034\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 4\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls34,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 4\n",
      "Spanish Ratio:  0.4883294144634972\n",
      "Korean Ratio:  0.4921559455593836\n",
      "Chinese Ratio:  0.5028331005271933\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 4\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls45,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 5\n",
      "Spanish Ratio:  0.5058779761904763\n",
      "Korean Ratio:  0.46396780217190303\n",
      "Chinese Ratio:  0.4691478942213516\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 5\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls45,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
