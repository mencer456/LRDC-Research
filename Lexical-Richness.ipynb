{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data analysis for Lexical Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary search for strings\n",
    "def binarySearchArr(arr, x):\n",
    "    l = 0\n",
    "    r = len(arr)\n",
    "    while (l <= r):\n",
    "        m = l + ((r - l) // 2)\n",
    " \n",
    "        res = (x == arr[m])\n",
    "        # Check if x is present at mid\n",
    "        if (res == True):\n",
    "            return m\n",
    " \n",
    "        # If x greater, ignore left half\n",
    "    #problem is here\n",
    "        elif (res == False):\n",
    "            if (x>arr[m]):\n",
    "                l = m+1\n",
    "            elif (x<arr[m]):\n",
    "                r=m-1\n",
    "        # If x is smaller, ignore right half\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function removes all of the rows that contain a string in the passed column\n",
    "def rmStr (df, col):\n",
    "    row_index=0\n",
    "    row_ind=[]\n",
    "    for i in df[col]:\n",
    "        try:\n",
    "            int(i)\n",
    "        except:\n",
    "            row_ind.append(row_index)\n",
    "        row_index+=1\n",
    "    return df.drop(labels=row_ind,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the unique values in a list\n",
    "def get_uniques(x):\n",
    "    return list(x.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the number of texts per language in the given list of anon_ids\n",
    "def find_counts(df,anon_list,lang_col):\n",
    "    spanish=0\n",
    "    chinese=0\n",
    "    korean=0\n",
    "    index=0\n",
    "    for i in df[\"Number of Questions\"]:\n",
    "        anon_id=str(df.at[index,'anon_id'])\n",
    "        if anon_id in anon_list:\n",
    "            if df.at[index,lang_col]=='Spanish':            \n",
    "                spanish+=i\n",
    "            elif df.at[index,lang_col]=='Chinese':\n",
    "                chinese+=i\n",
    "            elif df.at[index,lang_col]=='Korean':\n",
    "                korean+=i\n",
    "        index+=1\n",
    "    return(spanish,chinese,korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenAvg(df,textlencol,lang, id_list,lvl):\n",
    "    index=0\n",
    "    spanish=0\n",
    "    chinese=0\n",
    "    korean=0\n",
    "    qcounts=0\n",
    "    qcountc=0\n",
    "    qcountk=0\n",
    "\n",
    "    for anon in df['anon_id']:\n",
    "        if df.at[index,'level_id']==lvl and df.at[index,'anon_id'] in id_list:\n",
    "            textlen=df.at[index,textlencol]\n",
    "            textlen=int(textlen)\n",
    "            if df.at[index,lang]=='Spanish':\n",
    "                spanish+=textlen\n",
    "                qcounts+=1\n",
    "            elif df.at[index,lang]=='Korean':\n",
    "                korean+=textlen\n",
    "                qcountk+=1\n",
    "            elif df.at[index,lang]=='Chinese':\n",
    "                chinese+=textlen\n",
    "                qcountc+=1\n",
    "        index+=1\n",
    "        \n",
    "    print(\"Spanish Average: \",spanish/qcounts)\n",
    "    print(\"Korean Average: \",korean/qcountk)\n",
    "    print(\"Chinese Average: \",chinese/qcountc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(df,textlencol,typelencol,lang, id_list,lvl):\n",
    "    index=0\n",
    "    spanish=0\n",
    "    chinese=0\n",
    "    korean=0\n",
    "    tspanish=0\n",
    "    tkorean=0\n",
    "    tchinese=0\n",
    "    qcounts=0\n",
    "    qcountc=0\n",
    "    qcountk=0\n",
    "\n",
    "    for anon in df['anon_id']:\n",
    "        if df.at[index,'level_id']==lvl and df.at[index,'anon_id'] in id_list:\n",
    "            textlen=df.at[index,textlencol]\n",
    "            textlen=int(textlen)\n",
    "            typelen=df.at[index,typelencol]\n",
    "            typelen=int(typelen)\n",
    "            if df.at[index,lang]=='Spanish':\n",
    "                spanish+=textlen\n",
    "                tspanish+=typelen\n",
    "                qcounts+=1\n",
    "            elif df.at[index,lang]=='Korean':\n",
    "                korean+=textlen\n",
    "                tkorean+=typelen\n",
    "                qcountk+=1\n",
    "            elif df.at[index,lang]=='Chinese':\n",
    "                chinese+=textlen\n",
    "                tchinese+=typelen\n",
    "                qcountc+=1\n",
    "        index+=1\n",
    "        \n",
    "    spanishavg=spanish/qcounts\n",
    "    tspanishavg=tspanish/qcounts\n",
    "    koreanavg=korean/qcountk\n",
    "    tkoreanavg=tkorean/qcountk\n",
    "    chineseavg=chinese/qcountc\n",
    "    tchineseavg=tchinese/qcountc\n",
    "    \n",
    "    print(\"Spanish Ratio: \",tspanishavg/spanishavg)\n",
    "    print(\"Korean Ratio: \",tkoreanavg/koreanavg)\n",
    "    print(\"Chinese Ratio: \",tchineseavg/chineseavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that counts the amount of tokens per language FINDS AVERAGE OF ALL LVLS IN SPECIFIED LIST\n",
    "def finding_tokens(df,id_list,lang_col,text_len_col,qcounts):\n",
    "    spanish=0\n",
    "    korean=0\n",
    "    chinese=0\n",
    "    index=0\n",
    "    lspanish=0\n",
    "    lchinese=0\n",
    "    lkorean=0\n",
    "    \n",
    "    for i in df['anon_id']:\n",
    "        if i in id_list:\n",
    "            lang=df.at[index,lang_col]\n",
    "            length=int(df.at[index,text_len_col])\n",
    "            if lang=='Spanish':\n",
    "                lspanish+=1\n",
    "                spanish+=length\n",
    "            elif lang=='Korean':\n",
    "                lkorean+=1\n",
    "                korean+=length\n",
    "            elif lang=='Chinese':\n",
    "                lchinese+=1\n",
    "                chinese+=length\n",
    "        index+=1\n",
    "   # print('Spanish Token Average: ',spanish,'\\nKorean Token Average: ',korean,\n",
    "    #      '\\nChinese Token Average:',chinese)\n",
    "    print('Spanish Token Average: ',spanish/lspanish,'\\nKorean Token Average: ',korean/lkorean,\n",
    "          '\\nChinese Token Average:',chinese/lchinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loose=pd.read_csv('original-sheets\\Responses-Loose.csv')\n",
    "student_info=pd.read_csv('corpus-files\\student_information.csv')\n",
    "course=pd.read_csv('corpus-files\\course.csv')\n",
    "answer_df=pd.read_csv('answer.csv',index_col = 'answer_id', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_=course.drop(columns=['class_id','semester','section'])\n",
    "\n",
    "student=student_info.drop(columns=['birth_year','language_used_at_home','non_native_language_1','yrs_of_study_lang1',\n",
    "                                  'study_in_classroom_lang1','ways_of_study_lang1','non_native_language_2','study_in_classroom_lang2',\n",
    "                                  'ways_of_study_lang2','non_native_language_3','yrs_of_study_lang3','study_in_classroom_lang3',\n",
    "                                  'ways_of_study_lang3','yrs_of_english_learning','yrs_in_english_environment','yrs_of_study_lang2'])\n",
    "answer_=answer_df[['question_id','anon_id','course_id','version','text_len','text','tokens']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3Langs</th>\n",
       "      <th>Item</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text1_len</th>\n",
       "      <th>text2_len</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2 (line breaks/extra spaces removed, spaces added to reach 60)</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "      <th>Judgement</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>C12</td>\n",
       "      <td>141.0</td>\n",
       "      <td>aj8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>\\n\\n\\n Today, I am going to describe one of my...</td>\n",
       "      <td>Today, I am going to describe one of my classm...</td>\n",
       "      <td>Today, I am going to describe one of my classm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>C13</td>\n",
       "      <td>143.0</td>\n",
       "      <td>az8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Female</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>K9</td>\n",
       "      <td>133.0</td>\n",
       "      <td>az2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>K11</td>\n",
       "      <td>135.0</td>\n",
       "      <td>at8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>K20</td>\n",
       "      <td>188.0</td>\n",
       "      <td>eh9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  3Langs Item  answer_id anon_id       L1  gender  course_id  level_id  \\\n",
       "0      Y  C12      141.0     aj8  Chinese    Male      118.0       5.0   \n",
       "1      Y  C13      143.0     az8  Chinese  Female      118.0       5.0   \n",
       "2      Y   K9      133.0     az2   Korean    Male      118.0       5.0   \n",
       "3      Y  K11      135.0     at8   Korean  Female      118.0       5.0   \n",
       "4      Y  K20      188.0     eh9   Korean    Male      118.0       5.0   \n",
       "\n",
       "  class_id  question_id  version  text1_len  text2_len  text3_len  \\\n",
       "0        w         17.0      1.0      121.0      114.0      114.0   \n",
       "1        w         17.0      1.0       96.0       95.0       95.0   \n",
       "2        w         17.0      1.0      130.0      128.0      128.0   \n",
       "3        w         17.0      3.0      104.0      105.0      105.0   \n",
       "4        w         17.0      1.0       98.0       97.0       97.0   \n",
       "\n",
       "                                               text1  \\\n",
       "0  \\n\\n\\n Today, I am going to describe one of my...   \n",
       "1  My niece is 3 years old who is my younger brot...   \n",
       "2  When I was in Germany, I met a friend who was ...   \n",
       "3  my friend is ANON_NAME_0. she is a my ELI frie...   \n",
       "4  Younghun is my best friend. His facial appeara...   \n",
       "\n",
       "  text2 (line breaks/extra spaces removed, spaces added to reach 60)  \\\n",
       "0  Today, I am going to describe one of my classm...                   \n",
       "1  My niece is 3 years old who is my younger brot...                   \n",
       "2  When I was in Germany, I met a friend who was ...                   \n",
       "3  my friend is ANON_NAME_0. she is a my ELI frie...                   \n",
       "4  Younghun is my best friend. His facial appeara...                   \n",
       "\n",
       "               text3 (edits made to fix word counts)  Judgement Notes  \n",
       "0  Today, I am going to describe one of my classm...        1.0   NaN  \n",
       "1  My niece is 3 years old who is my younger brot...        1.0   NaN  \n",
       "2  When I was in Germany, I met a friend who was ...        1.0   NaN  \n",
       "3  my friend is ANON_NAME_0. she is a my ELI frie...        1.0   NaN  \n",
       "4  Younghun is my best friend. His facial appeara...        1.0   NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the answer df\n",
    "col='course_id'\n",
    "answer=rmStr(answer_, col) #removes strings from answer_.course_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the datasets\n",
    "\n",
    "merge_ans=answer[['anon_id','course_id']] #sets answer df up for merging \n",
    "student_merge=student.drop(columns=['course_history']) #set student df up for merging\n",
    "\n",
    "student_ans=student_merge.merge(answer,on='anon_id').astype({'course_id':'int64'}) #merges student and answers\n",
    "\n",
    "stu_ans_crs_=student_ans.merge(course_, on='course_id') #merges the student-answers df with course df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>anon_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>native_language</th>\n",
       "      <th>age</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>course_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>level_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>Some people prefer eat out and some like doing...</td>\n",
       "      <td>['Some', 'people', 'prefer', 'eat', 'out', 'an...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>97</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>288</td>\n",
       "      <td>My opinion is that a person does need educatio...</td>\n",
       "      <td>['My', 'opinion', 'is', 'that', 'a', 'person',...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1957</td>\n",
       "      <td>189</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>There are two national rooms in the Cathedral ...</td>\n",
       "      <td>['There', 'are', 'two', 'national', 'rooms', '...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2164</td>\n",
       "      <td>190</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>464</td>\n",
       "      <td>There are two nation rooms in the Cathedral of...</td>\n",
       "      <td>['There', 'are', 'two', 'nation', 'rooms', 'in...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bv5</td>\n",
       "      <td>Male</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21.0</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>\"Not all learning takes place in the classroom...</td>\n",
       "      <td>['``', 'Not', 'all', 'learning', 'takes', 'pla...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46225</th>\n",
       "      <td>46225</td>\n",
       "      <td>cy7</td>\n",
       "      <td>Female</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47682</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1. emphasis\\n2. appropriate\\n3. requires\\n4. a...</td>\n",
       "      <td>['1', '.', 'emphasis', '2', '.', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46226</th>\n",
       "      <td>46226</td>\n",
       "      <td>fp7</td>\n",
       "      <td>Female</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47823</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.emphasis \\n2.appropriate\\n3.requires\\n4.anal...</td>\n",
       "      <td>['1', '.', 'emphasis', '2', '.', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46227</th>\n",
       "      <td>46227</td>\n",
       "      <td>fq6</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47824</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.emphasis\\n2.appropriate\\n3.requires\\n4.analy...</td>\n",
       "      <td>['1', '.', 'emphasis', '2', '.', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46228</th>\n",
       "      <td>46228</td>\n",
       "      <td>di6</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47787</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1 emphasis\\n2 normal\\n3 requires\\n4 analyze\\n5...</td>\n",
       "      <td>['1', 'emphasis', '2', 'normal', '3', 'require...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46229</th>\n",
       "      <td>46229</td>\n",
       "      <td>fm3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47725</td>\n",
       "      <td>6066</td>\n",
       "      <td>1034</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1- emphasis\\n2- appropriate\\n3- requires\\n4- a...</td>\n",
       "      <td>['1', '-', 'emphasis', '2', '-', 'appropriate'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46230 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index anon_id   gender native_language   age answer_id question_id  \\\n",
       "0          0     do6   Female         Russian  31.0       150           4   \n",
       "1          1     do6   Female         Russian  31.0      1221          97   \n",
       "2          2     do6   Female         Russian  31.0      1957         189   \n",
       "3          3     do6   Female         Russian  31.0      2164         190   \n",
       "4          4     bv5     Male          Arabic  21.0       151           4   \n",
       "...      ...     ...      ...             ...   ...       ...         ...   \n",
       "46225  46225     cy7   Female          Korean   NaN     47682        6066   \n",
       "46226  46226     fp7   Female         Turkish   NaN     47823        6066   \n",
       "46227  46227     fq6     Male         Chinese   NaN     47824        6066   \n",
       "46228  46228     di6     Male         Chinese   NaN     47787        6066   \n",
       "46229  46229     fm3  Unknown          Arabic   NaN     47725        6066   \n",
       "\n",
       "       course_id version text_len  \\\n",
       "0            117       1      299   \n",
       "1            117       1      288   \n",
       "2            117       1      321   \n",
       "3            117       1      464   \n",
       "4            117       1      315   \n",
       "...          ...     ...      ...   \n",
       "46225       1034       1       10   \n",
       "46226       1034       1       10   \n",
       "46227       1034       1       10   \n",
       "46228       1034       1       10   \n",
       "46229       1034       1       10   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Some people prefer eat out and some like doing...   \n",
       "1      My opinion is that a person does need educatio...   \n",
       "2      There are two national rooms in the Cathedral ...   \n",
       "3      There are two nation rooms in the Cathedral of...   \n",
       "4      \"Not all learning takes place in the classroom...   \n",
       "...                                                  ...   \n",
       "46225  1. emphasis\\n2. appropriate\\n3. requires\\n4. a...   \n",
       "46226  1.emphasis \\n2.appropriate\\n3.requires\\n4.anal...   \n",
       "46227  1.emphasis\\n2.appropriate\\n3.requires\\n4.analy...   \n",
       "46228  1 emphasis\\n2 normal\\n3 requires\\n4 analyze\\n5...   \n",
       "46229  1- emphasis\\n2- appropriate\\n3- requires\\n4- a...   \n",
       "\n",
       "                                                  tokens  level_id  \n",
       "0      ['Some', 'people', 'prefer', 'eat', 'out', 'an...         5  \n",
       "1      ['My', 'opinion', 'is', 'that', 'a', 'person',...         5  \n",
       "2      ['There', 'are', 'two', 'national', 'rooms', '...         5  \n",
       "3      ['There', 'are', 'two', 'nation', 'rooms', 'in...         5  \n",
       "4      ['``', 'Not', 'all', 'learning', 'takes', 'pla...         5  \n",
       "...                                                  ...       ...  \n",
       "46225  ['1', '.', 'emphasis', '2', '.', 'appropriate'...         3  \n",
       "46226  ['1', '.', 'emphasis', '2', '.', 'appropriate'...         3  \n",
       "46227  ['1', '.', 'emphasis', '2', '.', 'appropriate'...         3  \n",
       "46228  ['1', 'emphasis', '2', 'normal', '3', 'require...         3  \n",
       "46229  ['1', '-', 'emphasis', '2', '-', 'appropriate'...         3  \n",
       "\n",
       "[46230 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans_crs_.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>anon_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>native_language</th>\n",
       "      <th>age</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>course_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>level_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>Some people prefer eat out and some like doing...</td>\n",
       "      <td>['Some', 'people', 'prefer', 'eat', 'out', 'an...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1221</td>\n",
       "      <td>97</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>288</td>\n",
       "      <td>My opinion is that a person does need educatio...</td>\n",
       "      <td>['My', 'opinion', 'is', 'that', 'a', 'person',...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1957</td>\n",
       "      <td>189</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>321</td>\n",
       "      <td>There are two national rooms in the Cathedral ...</td>\n",
       "      <td>['There', 'are', 'two', 'national', 'rooms', '...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>do6</td>\n",
       "      <td>Female</td>\n",
       "      <td>Russian</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2164</td>\n",
       "      <td>190</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>464</td>\n",
       "      <td>There are two nation rooms in the Cathedral of...</td>\n",
       "      <td>['There', 'are', 'two', 'nation', 'rooms', 'in...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bv5</td>\n",
       "      <td>Male</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>21.0</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>117</td>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>\"Not all learning takes place in the classroom...</td>\n",
       "      <td>['``', 'Not', 'all', 'learning', 'takes', 'pla...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17252</th>\n",
       "      <td>46214</td>\n",
       "      <td>bh8</td>\n",
       "      <td>Male</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48034</td>\n",
       "      <td>6087</td>\n",
       "      <td>1033</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>I received my Medical Doctor license in Japan ...</td>\n",
       "      <td>['I', 'received', 'my', 'Medical', 'Doctor', '...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17253</th>\n",
       "      <td>46215</td>\n",
       "      <td>bh8</td>\n",
       "      <td>Male</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48293</td>\n",
       "      <td>6119</td>\n",
       "      <td>1033</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>I introduce my ideal home to you quickly. This...</td>\n",
       "      <td>['I', 'introduce', 'my', 'ideal', 'home', 'to'...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17254</th>\n",
       "      <td>46217</td>\n",
       "      <td>cz3</td>\n",
       "      <td>Female</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47451</td>\n",
       "      <td>6027</td>\n",
       "      <td>1033</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>I will put 5 items in time capsule such as som...</td>\n",
       "      <td>['I', 'will', 'put', '5', 'items', 'in', 'time...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17255</th>\n",
       "      <td>46218</td>\n",
       "      <td>cz3</td>\n",
       "      <td>Female</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48001</td>\n",
       "      <td>6087</td>\n",
       "      <td>1033</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>I have studied Visual Design since 2002. I gra...</td>\n",
       "      <td>['I', 'have', 'studied', 'Visual', 'Design', '...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17256</th>\n",
       "      <td>46219</td>\n",
       "      <td>cz3</td>\n",
       "      <td>Female</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48316</td>\n",
       "      <td>6119</td>\n",
       "      <td>1033</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>I will describe about my ideal home. I want qu...</td>\n",
       "      <td>['I', 'will', 'describe', 'about', 'my', 'idea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17257 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index anon_id  gender native_language   age answer_id question_id  \\\n",
       "0          0     do6  Female         Russian  31.0       150           4   \n",
       "1          1     do6  Female         Russian  31.0      1221          97   \n",
       "2          2     do6  Female         Russian  31.0      1957         189   \n",
       "3          3     do6  Female         Russian  31.0      2164         190   \n",
       "4          4     bv5    Male          Arabic  21.0       151           4   \n",
       "...      ...     ...     ...             ...   ...       ...         ...   \n",
       "17252  46214     bh8    Male        Japanese   NaN     48034        6087   \n",
       "17253  46215     bh8    Male        Japanese   NaN     48293        6119   \n",
       "17254  46217     cz3  Female          Korean   NaN     47451        6027   \n",
       "17255  46218     cz3  Female          Korean   NaN     48001        6087   \n",
       "17256  46219     cz3  Female          Korean   NaN     48316        6119   \n",
       "\n",
       "       course_id version text_len  \\\n",
       "0            117       1      299   \n",
       "1            117       1      288   \n",
       "2            117       1      321   \n",
       "3            117       1      464   \n",
       "4            117       1      315   \n",
       "...          ...     ...      ...   \n",
       "17252       1033       1      100   \n",
       "17253       1033       1      138   \n",
       "17254       1033       1      101   \n",
       "17255       1033       1      100   \n",
       "17256       1033       1       72   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Some people prefer eat out and some like doing...   \n",
       "1      My opinion is that a person does need educatio...   \n",
       "2      There are two national rooms in the Cathedral ...   \n",
       "3      There are two nation rooms in the Cathedral of...   \n",
       "4      \"Not all learning takes place in the classroom...   \n",
       "...                                                  ...   \n",
       "17252  I received my Medical Doctor license in Japan ...   \n",
       "17253  I introduce my ideal home to you quickly. This...   \n",
       "17254  I will put 5 items in time capsule such as som...   \n",
       "17255  I have studied Visual Design since 2002. I gra...   \n",
       "17256  I will describe about my ideal home. I want qu...   \n",
       "\n",
       "                                                  tokens  level_id  \n",
       "0      ['Some', 'people', 'prefer', 'eat', 'out', 'an...         5  \n",
       "1      ['My', 'opinion', 'is', 'that', 'a', 'person',...         5  \n",
       "2      ['There', 'are', 'two', 'national', 'rooms', '...         5  \n",
       "3      ['There', 'are', 'two', 'nation', 'rooms', 'in...         5  \n",
       "4      ['``', 'Not', 'all', 'learning', 'takes', 'pla...         5  \n",
       "...                                                  ...       ...  \n",
       "17252  ['I', 'received', 'my', 'Medical', 'Doctor', '...         3  \n",
       "17253  ['I', 'introduce', 'my', 'ideal', 'home', 'to'...         3  \n",
       "17254  ['I', 'will', 'put', '5', 'items', 'in', 'time...         3  \n",
       "17255  ['I', 'have', 'studied', 'Visual', 'Design', '...         3  \n",
       "17256  ['I', 'will', 'describe', 'about', 'my', 'idea...         3  \n",
       "\n",
       "[17257 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=0\n",
    "ind_list=[]\n",
    "for i in stu_ans_crs_.text_len:\n",
    "    txtlen=int(i)\n",
    "    if txtlen < 60:\n",
    "        ind_list.append(index)\n",
    "    index+=1\n",
    "\n",
    "test=stu_ans_crs_.drop(labels=ind_list,axis=0).reset_index()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Answers in Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding spaces after punctuation if needed\n",
    "\n",
    "index=0\n",
    "stu_ans_crs=stu_ans_crs_.copy().drop(columns=['gender','age','question_id','course_id',\n",
    "                                             'version','tokens'])\n",
    "stu_ans_crs['processed_text']=pd.NaT\n",
    "stu_ans_crs['wordtype_len']=pd.NaT\n",
    "words=[]\n",
    "for text in stu_ans_crs.text:\n",
    "    uwords=[]\n",
    "    text=text.replace('\\n',' ')\n",
    "    nopunc=text.translate(str.maketrans('','',string.punctuation))\n",
    "    nopunc=nopunc.lower()\n",
    "    stu_ans_crs.at[index,'processed_text']=nopunc\n",
    "    words=nopunc.strip().split(\" \")\n",
    "    for word in words:\n",
    "        if word=='':\n",
    "            words.remove(word)\n",
    "        elif word not in uwords:\n",
    "            uwords.append(word)\n",
    "    stu_ans_crs.at[index,'text_len']=len(words)\n",
    "    stu_ans_crs.at[index,'wordtype_len']=len(uwords)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>level_id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>wordtype_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>150</td>\n",
       "      <td>299</td>\n",
       "      <td>Some people prefer eat out and some like doing...</td>\n",
       "      <td>5</td>\n",
       "      <td>some people prefer eat out and some like doing...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1221</td>\n",
       "      <td>288</td>\n",
       "      <td>My opinion is that a person does need educatio...</td>\n",
       "      <td>5</td>\n",
       "      <td>my opinion is that a person does need educatio...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1957</td>\n",
       "      <td>317</td>\n",
       "      <td>There are two national rooms in the Cathedral ...</td>\n",
       "      <td>5</td>\n",
       "      <td>there are two national rooms in the cathedral ...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>2164</td>\n",
       "      <td>459</td>\n",
       "      <td>There are two nation rooms in the Cathedral of...</td>\n",
       "      <td>5</td>\n",
       "      <td>there are two nation rooms in the cathedral of...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bv5</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>151</td>\n",
       "      <td>311</td>\n",
       "      <td>\"Not all learning takes place in the classroom...</td>\n",
       "      <td>5</td>\n",
       "      <td>not all learning takes place in the classroom ...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46225</th>\n",
       "      <td>cy7</td>\n",
       "      <td>Korean</td>\n",
       "      <td>47682</td>\n",
       "      <td>20</td>\n",
       "      <td>1. emphasis\\n2. appropriate\\n3. requires\\n4. a...</td>\n",
       "      <td>3</td>\n",
       "      <td>1 emphasis 2 appropriate 3 requires 4 analyze ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46226</th>\n",
       "      <td>fp7</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>47823</td>\n",
       "      <td>10</td>\n",
       "      <td>1.emphasis \\n2.appropriate\\n3.requires\\n4.anal...</td>\n",
       "      <td>3</td>\n",
       "      <td>1emphasis  2appropriate 3requires 4analyze 5au...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46227</th>\n",
       "      <td>fq6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>47824</td>\n",
       "      <td>10</td>\n",
       "      <td>1.emphasis\\n2.appropriate\\n3.requires\\n4.analy...</td>\n",
       "      <td>3</td>\n",
       "      <td>1emphasis 2appropriate 3requires 4analyze 5aut...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46228</th>\n",
       "      <td>di6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>47787</td>\n",
       "      <td>20</td>\n",
       "      <td>1 emphasis\\n2 normal\\n3 requires\\n4 analyze\\n5...</td>\n",
       "      <td>3</td>\n",
       "      <td>1 emphasis 2 normal 3 requires 4 analyze 5 aut...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46229</th>\n",
       "      <td>fm3</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>47725</td>\n",
       "      <td>20</td>\n",
       "      <td>1- emphasis\\n2- appropriate\\n3- requires\\n4- a...</td>\n",
       "      <td>3</td>\n",
       "      <td>1 emphasis 2 appropriate 3 requires 4 analyze ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46230 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anon_id native_language answer_id text_len  \\\n",
       "0         do6         Russian       150      299   \n",
       "1         do6         Russian      1221      288   \n",
       "2         do6         Russian      1957      317   \n",
       "3         do6         Russian      2164      459   \n",
       "4         bv5          Arabic       151      311   \n",
       "...       ...             ...       ...      ...   \n",
       "46225     cy7          Korean     47682       20   \n",
       "46226     fp7         Turkish     47823       10   \n",
       "46227     fq6         Chinese     47824       10   \n",
       "46228     di6         Chinese     47787       20   \n",
       "46229     fm3          Arabic     47725       20   \n",
       "\n",
       "                                                    text  level_id  \\\n",
       "0      Some people prefer eat out and some like doing...         5   \n",
       "1      My opinion is that a person does need educatio...         5   \n",
       "2      There are two national rooms in the Cathedral ...         5   \n",
       "3      There are two nation rooms in the Cathedral of...         5   \n",
       "4      \"Not all learning takes place in the classroom...         5   \n",
       "...                                                  ...       ...   \n",
       "46225  1. emphasis\\n2. appropriate\\n3. requires\\n4. a...         3   \n",
       "46226  1.emphasis \\n2.appropriate\\n3.requires\\n4.anal...         3   \n",
       "46227  1.emphasis\\n2.appropriate\\n3.requires\\n4.analy...         3   \n",
       "46228  1 emphasis\\n2 normal\\n3 requires\\n4 analyze\\n5...         3   \n",
       "46229  1- emphasis\\n2- appropriate\\n3- requires\\n4- a...         3   \n",
       "\n",
       "                                          processed_text wordtype_len  \n",
       "0      some people prefer eat out and some like doing...          125  \n",
       "1      my opinion is that a person does need educatio...          112  \n",
       "2      there are two national rooms in the cathedral ...          149  \n",
       "3      there are two nation rooms in the cathedral of...          187  \n",
       "4      not all learning takes place in the classroom ...          136  \n",
       "...                                                  ...          ...  \n",
       "46225  1 emphasis 2 appropriate 3 requires 4 analyze ...           20  \n",
       "46226  1emphasis  2appropriate 3requires 4analyze 5au...            9  \n",
       "46227  1emphasis 2appropriate 3requires 4analyze 5aut...           10  \n",
       "46228  1 emphasis 2 normal 3 requires 4 analyze 5 aut...           20  \n",
       "46229  1 emphasis 2 appropriate 3 requires 4 analyze ...           20  \n",
       "\n",
       "[46230 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all rows where the answer is less than 60 words\n",
    "index=0\n",
    "ind_list=[]\n",
    "for i in stu_ans_crs.text_len:\n",
    "    txtlen=int(i)\n",
    "    if txtlen < 60:\n",
    "        ind_list.append(index)\n",
    "    index+=1\n",
    "\n",
    "stu_ans_crs=stu_ans_crs.drop(labels=ind_list,axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>level_id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>wordtype_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>150</td>\n",
       "      <td>299</td>\n",
       "      <td>Some people prefer eat out and some like doing...</td>\n",
       "      <td>5</td>\n",
       "      <td>some people prefer eat out and some like doing...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1221</td>\n",
       "      <td>288</td>\n",
       "      <td>My opinion is that a person does need educatio...</td>\n",
       "      <td>5</td>\n",
       "      <td>my opinion is that a person does need educatio...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1957</td>\n",
       "      <td>317</td>\n",
       "      <td>There are two national rooms in the Cathedral ...</td>\n",
       "      <td>5</td>\n",
       "      <td>there are two national rooms in the cathedral ...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>do6</td>\n",
       "      <td>Russian</td>\n",
       "      <td>2164</td>\n",
       "      <td>459</td>\n",
       "      <td>There are two nation rooms in the Cathedral of...</td>\n",
       "      <td>5</td>\n",
       "      <td>there are two nation rooms in the cathedral of...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bv5</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>151</td>\n",
       "      <td>311</td>\n",
       "      <td>\"Not all learning takes place in the classroom...</td>\n",
       "      <td>5</td>\n",
       "      <td>not all learning takes place in the classroom ...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17144</th>\n",
       "      <td>46214</td>\n",
       "      <td>bh8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>48034</td>\n",
       "      <td>107</td>\n",
       "      <td>I received my Medical Doctor license in Japan ...</td>\n",
       "      <td>3</td>\n",
       "      <td>i received my medical doctor license in japan ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17145</th>\n",
       "      <td>46215</td>\n",
       "      <td>bh8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>48293</td>\n",
       "      <td>138</td>\n",
       "      <td>I introduce my ideal home to you quickly. This...</td>\n",
       "      <td>3</td>\n",
       "      <td>i introduce my ideal home to you quickly this ...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17146</th>\n",
       "      <td>46217</td>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>47451</td>\n",
       "      <td>102</td>\n",
       "      <td>I will put 5 items in time capsule such as som...</td>\n",
       "      <td>3</td>\n",
       "      <td>i will put 5 items in time capsule such as som...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17147</th>\n",
       "      <td>46218</td>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>48001</td>\n",
       "      <td>104</td>\n",
       "      <td>I have studied Visual Design since 2002. I gra...</td>\n",
       "      <td>3</td>\n",
       "      <td>i have studied visual design since 2002 i grad...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17148</th>\n",
       "      <td>46219</td>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>48316</td>\n",
       "      <td>70</td>\n",
       "      <td>I will describe about my ideal home. I want qu...</td>\n",
       "      <td>3</td>\n",
       "      <td>i will describe about my ideal home i want qui...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17149 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index anon_id native_language answer_id text_len  \\\n",
       "0          0     do6         Russian       150      299   \n",
       "1          1     do6         Russian      1221      288   \n",
       "2          2     do6         Russian      1957      317   \n",
       "3          3     do6         Russian      2164      459   \n",
       "4          4     bv5          Arabic       151      311   \n",
       "...      ...     ...             ...       ...      ...   \n",
       "17144  46214     bh8        Japanese     48034      107   \n",
       "17145  46215     bh8        Japanese     48293      138   \n",
       "17146  46217     cz3          Korean     47451      102   \n",
       "17147  46218     cz3          Korean     48001      104   \n",
       "17148  46219     cz3          Korean     48316       70   \n",
       "\n",
       "                                                    text  level_id  \\\n",
       "0      Some people prefer eat out and some like doing...         5   \n",
       "1      My opinion is that a person does need educatio...         5   \n",
       "2      There are two national rooms in the Cathedral ...         5   \n",
       "3      There are two nation rooms in the Cathedral of...         5   \n",
       "4      \"Not all learning takes place in the classroom...         5   \n",
       "...                                                  ...       ...   \n",
       "17144  I received my Medical Doctor license in Japan ...         3   \n",
       "17145  I introduce my ideal home to you quickly. This...         3   \n",
       "17146  I will put 5 items in time capsule such as som...         3   \n",
       "17147  I have studied Visual Design since 2002. I gra...         3   \n",
       "17148  I will describe about my ideal home. I want qu...         3   \n",
       "\n",
       "                                          processed_text wordtype_len  \n",
       "0      some people prefer eat out and some like doing...          125  \n",
       "1      my opinion is that a person does need educatio...          112  \n",
       "2      there are two national rooms in the cathedral ...          149  \n",
       "3      there are two nation rooms in the cathedral of...          187  \n",
       "4      not all learning takes place in the classroom ...          136  \n",
       "...                                                  ...          ...  \n",
       "17144  i received my medical doctor license in japan ...           54  \n",
       "17145  i introduce my ideal home to you quickly this ...           88  \n",
       "17146  i will put 5 items in time capsule such as som...           67  \n",
       "17147  i have studied visual design since 2002 i grad...           57  \n",
       "17148  i will describe about my ideal home i want qui...           51  \n",
       "\n",
       "[17149 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_ans_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Loose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_loose=loose.drop(columns=['3Langs','Item','answer_id','gender','course_id','class_id','question_id','version','text1_len',\n",
    "                                     'text2_len','text1','text2 (line breaks/extra spaces removed, spaces added to reach 60)',\n",
    "                                     'Judgement','Notes'])\n",
    "drop_loose=dropped_loose.dropna()\n",
    "\n",
    "sorted_loose=drop_loose.sort_values(by=['anon_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>level_id</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Barber, chef by profession, but an expert on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>The article \"English as Co star\" support the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>In this article the authors Goleman, Kaufman a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Flow in the sense expressed in the text \"The C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Bill Gates, in this conference, explained two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>Pittsburgh had a big snowstorm recently. A lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Korea, which is connected to China, belongs to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>Koko is a big female gorilla who was born in S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>4.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>A natural disaster is the effect of the natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>I think human language and nonhuman primate la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anon_id       L1  level_id  text3_len  \\\n",
       "0        aa0  Spanish       5.0      109.0   \n",
       "1        aa0  Spanish       5.0      190.0   \n",
       "2        aa0  Spanish       5.0      193.0   \n",
       "3        aa0  Spanish       5.0      170.0   \n",
       "4        aa0  Spanish       5.0       85.0   \n",
       "...      ...      ...       ...        ...   \n",
       "1805     ha2   Korean       5.0      217.0   \n",
       "1806     ha2   Korean       5.0      170.0   \n",
       "1807     ha2   Korean       5.0      195.0   \n",
       "1808     ha2   Korean       4.0      283.0   \n",
       "1809     ha2   Korean       5.0      168.0   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  \n",
       "0     Barber, chef by profession, but an expert on a...  \n",
       "1     The article \"English as Co star\" support the f...  \n",
       "2     In this article the authors Goleman, Kaufman a...  \n",
       "3     Flow in the sense expressed in the text \"The C...  \n",
       "4     Bill Gates, in this conference, explained two ...  \n",
       "...                                                 ...  \n",
       "1805  Pittsburgh had a big snowstorm recently. A lot...  \n",
       "1806  Korea, which is connected to China, belongs to...  \n",
       "1807  Koko is a big female gorilla who was born in S...  \n",
       "1808  A natural disaster is the effect of the natura...  \n",
       "1809  I think human language and nonhuman primate la...  \n",
       "\n",
       "[1810 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "textcount_loose=sorted_loose.copy()\n",
    "\n",
    "textcount_loose['processed_text']=pd.NaT\n",
    "textcount_loose['wordtype_len']=pd.NaT\n",
    "words=[]\n",
    "for text in textcount_loose['text3 (edits made to fix word counts)']:\n",
    "    uwords=[]\n",
    "    text=text.replace('\\n',' ')\n",
    "    nopunc=text.translate(str.maketrans('','',string.punctuation))\n",
    "    nopunc=nopunc.lower()\n",
    "    textcount_loose.at[index,'processed_text']=nopunc\n",
    "    words=nopunc.strip().split(\" \")\n",
    "    for word in words:\n",
    "        if word=='':\n",
    "            words.remove(word)\n",
    "        elif word not in uwords:\n",
    "            uwords.append(word)\n",
    "    textcount_loose.at[index,'text3_len']=len(words)\n",
    "    textcount_loose.at[index,'wordtype_len']=len(uwords)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>level_id</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>wordtype_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Barber, chef by profession, but an expert on a...</td>\n",
       "      <td>barber chef by profession but an expert on agr...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>The article \"English as Co star\" support the f...</td>\n",
       "      <td>the article english as co star support the fac...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>In this article the authors Goleman, Kaufman a...</td>\n",
       "      <td>in this article the authors goleman kaufman an...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Flow in the sense expressed in the text \"The C...</td>\n",
       "      <td>flow in the sense expressed in the text the cr...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Bill Gates, in this conference, explained two ...</td>\n",
       "      <td>bill gates in this conference explained two bi...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>Pittsburgh had a big snowstorm recently. A lot...</td>\n",
       "      <td>pittsburgh had a big snowstorm recently a lot ...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Korea, which is connected to China, belongs to...</td>\n",
       "      <td>korea which is connected to china belongs to t...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>Koko is a big female gorilla who was born in S...</td>\n",
       "      <td>koko is a big female gorilla who was born in s...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>4.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>A natural disaster is the effect of the natura...</td>\n",
       "      <td>a natural disaster is the effect of the natura...</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>I think human language and nonhuman primate la...</td>\n",
       "      <td>i think human language and nonhuman primate la...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anon_id       L1  level_id  text3_len  \\\n",
       "0        aa0  Spanish       5.0      109.0   \n",
       "1        aa0  Spanish       5.0      190.0   \n",
       "2        aa0  Spanish       5.0      193.0   \n",
       "3        aa0  Spanish       5.0      170.0   \n",
       "4        aa0  Spanish       5.0       85.0   \n",
       "...      ...      ...       ...        ...   \n",
       "1805     ha2   Korean       5.0      217.0   \n",
       "1806     ha2   Korean       5.0      170.0   \n",
       "1807     ha2   Korean       5.0      195.0   \n",
       "1808     ha2   Korean       4.0      283.0   \n",
       "1809     ha2   Korean       5.0      168.0   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  \\\n",
       "0     Barber, chef by profession, but an expert on a...   \n",
       "1     The article \"English as Co star\" support the f...   \n",
       "2     In this article the authors Goleman, Kaufman a...   \n",
       "3     Flow in the sense expressed in the text \"The C...   \n",
       "4     Bill Gates, in this conference, explained two ...   \n",
       "...                                                 ...   \n",
       "1805  Pittsburgh had a big snowstorm recently. A lot...   \n",
       "1806  Korea, which is connected to China, belongs to...   \n",
       "1807  Koko is a big female gorilla who was born in S...   \n",
       "1808  A natural disaster is the effect of the natura...   \n",
       "1809  I think human language and nonhuman primate la...   \n",
       "\n",
       "                                         processed_text wordtype_len  \n",
       "0     barber chef by profession but an expert on agr...           79  \n",
       "1     the article english as co star support the fac...          117  \n",
       "2     in this article the authors goleman kaufman an...          122  \n",
       "3     flow in the sense expressed in the text the cr...          103  \n",
       "4     bill gates in this conference explained two bi...           66  \n",
       "...                                                 ...          ...  \n",
       "1805  pittsburgh had a big snowstorm recently a lot ...          129  \n",
       "1806  korea which is connected to china belongs to t...          101  \n",
       "1807  koko is a big female gorilla who was born in s...          115  \n",
       "1808  a natural disaster is the effect of the natura...          135  \n",
       "1809  i think human language and nonhuman primate la...          100  \n",
       "\n",
       "[1810 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcount_loose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Ids of participants in all three of Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_lvls=stu_ans_crs[['anon_id','native_language','level_id']]\n",
    "sac_lvls_rkc=sac_lvls\n",
    "\n",
    "index=0\n",
    "for i in sac_lvls_rkc.native_language:\n",
    "    if (i != 'Korean' and i != 'Spanish' and i != 'Chinese'):\n",
    "        sac_lvls_rkc=sac_lvls_rkc.drop(index)\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>level_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ax4</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17135</th>\n",
       "      <td>ew6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17136</th>\n",
       "      <td>ew6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17146</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17147</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17148</th>\n",
       "      <td>cz3</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7580 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      anon_id native_language  level_id\n",
       "13        ax4         Chinese         5\n",
       "14        ax4         Chinese         5\n",
       "15        ax4         Chinese         5\n",
       "16        ax4         Chinese         5\n",
       "17        ax4         Chinese         5\n",
       "...       ...             ...       ...\n",
       "17135     ew6         Chinese         3\n",
       "17136     ew6         Chinese         3\n",
       "17146     cz3          Korean         3\n",
       "17147     cz3          Korean         3\n",
       "17148     cz3          Korean         3\n",
       "\n",
       "[7580 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac_lvls_rkc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to see what levels each anon_id participated in \n",
    "lvl_list=sac_lvls_rkc.groupby('anon_id').agg(n_uniq=('level_id','nunique'), lvl_nums=('level_id',get_uniques))\n",
    "lvl_list=lvl_list.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>n_uniq</th>\n",
       "      <th>lvl_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa1</td>\n",
       "      <td>1</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa8</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa9</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>ha0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>ha2</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ha6</td>\n",
       "      <td>2</td>\n",
       "      <td>[3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>hb4</td>\n",
       "      <td>3</td>\n",
       "      <td>[5, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>hc1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id  n_uniq   lvl_nums\n",
       "0       aa0       1        [5]\n",
       "1       aa1       1        [4]\n",
       "2       aa3       1        [4]\n",
       "3       aa8       2     [5, 4]\n",
       "4       aa9       3  [3, 4, 2]\n",
       "..      ...     ...        ...\n",
       "470     ha0       1        [5]\n",
       "471     ha2       2     [5, 4]\n",
       "472     ha6       2     [3, 4]\n",
       "473     hb4       3  [5, 4, 3]\n",
       "474     hc1       1        [5]\n",
       "\n",
       "[475 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lvl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will create a list of only the ids that have levels 3, 4, and 5 in their lvl_nums column\n",
    "all_three=[]\n",
    "ind=0\n",
    "for i in lvl_list.anon_id:\n",
    "    if ((3 in lvl_list.iat[ind,2]) and (4 in lvl_list.iat[ind,2]) and (5 in lvl_list.iat[ind,2])):\n",
    "        append=lvl_list.at[ind, 'anon_id']\n",
    "        all_three.append(append)\n",
    "    ind+=1\n",
    "\n",
    "all_three.sort()\n",
    "len(all_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anon=[]\n",
    "for i in sac_lvls_rkc.anon_id:\n",
    "    all_anon.append(i)\n",
    "len(all_anon)\n",
    "all_anon.sort()\n",
    "\n",
    "sorted_sac=sac_lvls_rkc.sort_values(by=['anon_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['ag9', 'an5', 'as0', 'ay1', 'be7', 'bv9', 'bz5', 'cc4', 'cj8', 'co5', 'cv3', 'cw0', 'ea4', 'eq8', 'es9', 'fj7', 'fp5', 'fu6', 'gc2', 'gq8', 'gz2', 'hb4'], 'Chinese': ['aq1', 'ar8', 'bf7', 'bl4', 'bl7', 'bp4', 'bz2', 'cb3', 'cf9', 'cz4', 'dk6', 'ev9', 'fi1', 'fj4', 'gb4', 'gx5'], 'Spanish': ['bj2', 'cm9', 'fa2', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "lang_dict_all={}\n",
    "for i in all_three:\n",
    "    df_ind=binarySearchArr(all_anon,i)\n",
    "    key=sorted_sac.at[df_ind,'native_language']\n",
    "    val=sorted_sac.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in lang_dict_all:\n",
    "        lang_dict_all[key].append(val)\n",
    "    else:\n",
    "        lang_dict_all[key]=[val]\n",
    "    \n",
    "print(lang_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will create a list of only the ids that have levels 3 and 4 in their lvl_nums column\n",
    "lvls34=[]\n",
    "ind=0\n",
    "for i in lvl_list.anon_id:\n",
    "    if ((3 in lvl_list.iat[ind,2]) and (4 in lvl_list.iat[ind,2])):\n",
    "        append=lvl_list.at[ind, 'anon_id']\n",
    "        lvls34.append(append)\n",
    "    ind+=1\n",
    "\n",
    "lvls34.sort()\n",
    "len(lvls34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa9', 'af1', 'ag9', 'ah9', 'an4', 'an5', 'as0', 'as4', 'ay1', 'bc0', 'be2', 'be7', 'bf1', 'bm0', 'br5', 'bv9', 'bz5', 'ca6', 'cc4', 'cf6', 'ch9', 'cj8', 'cn3', 'co5', 'cv3', 'cw0', 'cw6', 'cy5', 'dd1', 'dj6', 'dp5', 'ea4', 'ef4', 'eo2', 'eq8', 'es9', 'et1', 'ex0', 'fc9', 'fh7', 'fi4', 'fj7', 'fk1', 'fl4', 'fl5', 'fo3', 'fp5', 'fp9', 'fu6', 'fv7', 'fv9', 'gb7', 'gc2', 'gd1', 'gl4', 'gq8', 'gv1', 'gz2', 'ha6', 'hb4'], 'Chinese': ['ag6', 'ai4', 'ap5', 'ap8', 'aq1', 'ar8', 'ax7', 'ba3', 'bf7', 'bl4', 'bl7', 'bp4', 'bu9', 'bz2', 'cb3', 'cf9', 'cl2', 'cl6', 'cq2', 'cs5', 'cz4', 'dk6', 'dl8', 'dm4', 'do7', 'dp1', 'dt4', 'dx1', 'eb6', 'ev5', 'ev9', 'ey8', 'ff6', 'fi1', 'fj4', 'fm2', 'fn8', 'fy3', 'gb4', 'gw5', 'gw8', 'gx5'], 'Spanish': ['bj2', 'ch0', 'cm9', 'en3', 'fa2', 'fe7', 'fg7', 'fy1', 'fy6']}\n"
     ]
    }
   ],
   "source": [
    "lang_dict_34={}\n",
    "for i in lvls34:\n",
    "    df_ind=binarySearchArr(all_anon,i)\n",
    "    key=sorted_sac.at[df_ind,'native_language']\n",
    "    val=sorted_sac.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in lang_dict_34:\n",
    "        lang_dict_34[key].append(val)\n",
    "    else:\n",
    "        lang_dict_34[key]=[val]\n",
    "    \n",
    "print(lang_dict_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will create a list of only the ids that have levels 3 and 4 in their lvl_nums column\n",
    "lvls45=[]\n",
    "ind=0\n",
    "for i in lvl_list.anon_id:\n",
    "    if ((4 in lvl_list.iat[ind,2]) and (5 in lvl_list.iat[ind,2])):\n",
    "        append=lvl_list.at[ind, 'anon_id']\n",
    "        lvls45.append(append)\n",
    "    ind+=1\n",
    "\n",
    "lvls45.sort()\n",
    "len(lvls45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa8', 'ac5', 'ad1', 'ag9', 'al5', 'an5', 'ap4', 'aq5', 'as0', 'as7', 'au5', 'au6', 'aw6', 'ay1', 'be7', 'bq0', 'bu4', 'bv9', 'bw3', 'bz5', 'cc4', 'ce3', 'ch2', 'ci0', 'cj8', 'co5', 'cp0', 'cv3', 'cw0', 'cw1', 'dd9', 'df3', 'dm3', 'dy7', 'ea4', 'eb3', 'eb9', 'ec1', 'eg5', 'eq8', 'es0', 'es9', 'et3', 'ex3', 'fa0', 'fb4', 'fd6', 'ff1', 'fi5', 'fj7', 'fl0', 'fp5', 'fs0', 'ft2', 'fu6', 'fx4', 'fz8', 'ga1', 'gc2', 'ge6', 'gg2', 'gg6', 'gj0', 'gk5', 'gl1', 'gn0', 'gq8', 'gs3', 'gu0', 'gz2', 'ha2', 'hb4'], 'Chinese': ['ad7', 'ae9', 'af4', 'am5', 'an7', 'aq1', 'ar8', 'ar9', 'az8', 'bd7', 'bf2', 'bf7', 'bf9', 'bl4', 'bl7', 'bp2', 'bp4', 'br9', 'bv8', 'bw9', 'by5', 'by6', 'bz1', 'bz2', 'ca4', 'cb3', 'cd6', 'cf9', 'ci2', 'cj5', 'cl3', 'cz2', 'cz4', 'dc0', 'di7', 'dk6', 'dm8', 'dq9', 'du9', 'dw2', 'ei2', 'ei8', 'eo1', 'eq4', 'es4', 'et4', 'ev6', 'ev9', 'ff2', 'fg8', 'fi1', 'fj4', 'fk8', 'fo4', 'fq5', 'fw7', 'ga3', 'gb4', 'gf5', 'gl2', 'gm1', 'go8', 'gt0', 'gv3', 'gw0', 'gx5'], 'Spanish': ['bi4', 'bj2', 'br2', 'cm9', 'cv7', 'dc6', 'df4', 'dk0', 'eo8', 'fa2', 'fx7', 'fy1', 'gq4', 'gs6', 'gu1', 'gu9']}\n"
     ]
    }
   ],
   "source": [
    "lang_dict_45={}\n",
    "for i in lvls45:\n",
    "    df_ind=binarySearchArr(all_anon,i)\n",
    "    key=sorted_sac.at[df_ind,'native_language']\n",
    "    val=sorted_sac.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in lang_dict_45:\n",
    "        lang_dict_45[key].append(val)\n",
    "    else:\n",
    "        lang_dict_45[key]=[val]\n",
    "    \n",
    "print(lang_dict_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-5:  42 \n",
      "3-4:  111 \n",
      "4-5:  154\n"
     ]
    }
   ],
   "source": [
    "print(\"3-5: \",len(all_three),'\\n3-4: ',len(lvls34),\"\\n4-5: \",len(lvls45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Three:\n",
      "Korean 22\n",
      "Chinese 16\n",
      "Spanish 4\n"
     ]
    }
   ],
   "source": [
    "print(\"All Three:\")\n",
    "for key, value in lang_dict_all.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-4:\n",
      "Korean 60\n",
      "Chinese 42\n",
      "Spanish 9\n"
     ]
    }
   ],
   "source": [
    "print(\"3-4:\")\n",
    "\n",
    "for key, value in lang_dict_34.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-5:\n",
      "Korean 72\n",
      "Chinese 66\n",
      "Spanish 16\n"
     ]
    }
   ],
   "source": [
    "print(\"4-5:\")\n",
    "\n",
    "for key, value in lang_dict_45.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting distributions of Loose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1810"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the columns that aren't needed for what we're doing right now\n",
    "\n",
    "all_anon_loose=[]\n",
    "for i in sorted_loose.anon_id:\n",
    "    all_anon_loose.append(i)\n",
    "len(all_anon_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>n_anon_ids_loose</th>\n",
       "      <th>lvl_nums_loose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa3</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa8</td>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa9</td>\n",
       "      <td>2</td>\n",
       "      <td>[3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ab6</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>gz2</td>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>gz5</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>gz7</td>\n",
       "      <td>1</td>\n",
       "      <td>[4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ha0</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>ha2</td>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id  n_anon_ids_loose lvl_nums_loose\n",
       "0       aa0                 1          [5.0]\n",
       "1       aa3                 1          [4.0]\n",
       "2       aa8                 2     [4.0, 5.0]\n",
       "3       aa9                 2     [3.0, 4.0]\n",
       "4       ab6                 1          [4.0]\n",
       "..      ...               ...            ...\n",
       "278     gz2                 2     [4.0, 5.0]\n",
       "279     gz5                 1          [4.0]\n",
       "280     gz7                 1          [4.0]\n",
       "281     ha0                 1          [5.0]\n",
       "282     ha2                 2     [4.0, 5.0]\n",
       "\n",
       "[283 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code gets how many times an id appeared (n_anon_ids) as well as an array of what levels they participated in (lvl_nums)\n",
    "clean_ids_lvl_loose=drop_loose.groupby('anon_id').agg(\n",
    "    n_anon_ids_loose=('level_id', 'nunique'),\n",
    "    lvl_nums_loose=('level_id', get_uniques)\n",
    ")\n",
    "\n",
    "#changes the clean_ids_lvl's index so we can access the anon_id easier\n",
    "ind_loose= clean_ids_lvl_loose.reset_index()\n",
    "ind_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for loop that checks if lvl_nums has all three levels in it (3,4, and 5)\n",
    "# if there are three values in lvl_nums, the anon_id is appended to the all_three list\n",
    "ind=0\n",
    "all_three_loose=[]\n",
    "append=''\n",
    "for i in ind_loose.anon_id:\n",
    "    if ind_loose.iat[ind, 1]==3:\n",
    "        append=ind_loose.at[ind,'anon_id']\n",
    "        all_three_loose.append(append)\n",
    "    ind=ind+1\n",
    "\n",
    "all_three_loose.sort()\n",
    "len(all_three_loose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# creates list of participants in loose sheet that are in level 3 and 4\n",
    "ind=0\n",
    "in34_loose=[]\n",
    "append=''\n",
    "for i in ind_loose.anon_id:\n",
    "    if ind_loose.iat[ind, 1]==2:\n",
    "        if (3 in ind_loose.iat[ind,2] and 4 in ind_loose.iat[ind,2]):\n",
    "            append=ind_loose.at[ind,'anon_id']\n",
    "            in34_loose.append(append)\n",
    "    elif ind_loose.iat[ind,1]==3:\n",
    "        append=ind_loose.at[ind,'anon_id']\n",
    "        in34_loose.append(append)\n",
    "    ind=ind+1\n",
    "in34_loose.sort()\n",
    "print(len(in34_loose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['an5', 'bv9'], 'Chinese': ['ar8', 'bl4', 'bp4', 'bz2'], 'Spanish': ['fa2', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "llang_dict_35={}\n",
    "for i in all_three_loose:\n",
    "    df_ind=binarySearchArr(all_anon_loose,i)\n",
    "    key=sorted_loose.at[df_ind,'L1']\n",
    "    val=sorted_loose.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in llang_dict_35:\n",
    "        llang_dict_35[key].append(val)\n",
    "    else:\n",
    "        llang_dict_35[key]=[val]\n",
    "    \n",
    "print(llang_dict_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa9', 'ah9', 'an5', 'be2', 'bf1', 'bv9', 'cf6', 'cw6', 'cy5', 'ex0', 'fc9', 'fv7', 'gd1', 'gv1'], 'Chinese': ['ap5', 'ar8', 'ax7', 'bl4', 'bp4', 'bz2', 'cs5', 'ey8', 'fn8'], 'Spanish': ['ch0', 'cm9', 'fa2', 'fe7', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "llang_dict_34={}\n",
    "for i in in34_loose:\n",
    "    df_ind=binarySearchArr(all_anon_loose,i)\n",
    "    key=sorted_loose.at[df_ind,'L1']\n",
    "    val=sorted_loose.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in llang_dict_34:\n",
    "        llang_dict_34[key].append(val)\n",
    "    else:\n",
    "        llang_dict_34[key]=[val]\n",
    "    \n",
    "print(llang_dict_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "# creates list of participants in loose sheet that are in level 4 and 5\n",
    "ind=0\n",
    "in45_loose=[]\n",
    "append=''\n",
    "for i in ind_loose.anon_id:\n",
    "    if ind_loose.iat[ind, 1]==2:\n",
    "        if (4 in ind_loose.iat[ind,2] and 5 in ind_loose.iat[ind,2]):\n",
    "            append=ind_loose.at[ind,'anon_id']\n",
    "            in45_loose.append(append)\n",
    "    elif ind_loose.iat[ind,1]==3:\n",
    "        append=ind_loose.at[ind,'anon_id']\n",
    "        in45_loose.append(append)\n",
    "    ind=ind+1\n",
    "in45_loose.sort()\n",
    "print(len(in45_loose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa8', 'ac5', 'al5', 'an5', 'au5', 'au6', 'bu4', 'bv9', 'cc4', 'ce3', 'cj8', 'co5', 'cw1', 'dd9', 'dy7', 'fa0', 'ff1', 'fj7', 'fp5', 'fu6', 'fx4', 'gq8', 'gz2', 'ha2'], 'Chinese': ['aq1', 'ar8', 'ar9', 'az8', 'bf2', 'bl4', 'bl7', 'bp4', 'bz2', 'cb3', 'cj5', 'cz4', 'dc0', 'dm8', 'dq9', 'ei8', 'et4', 'ev6', 'ev9', 'ff2', 'fg8', 'fk8', 'gb4', 'gf5', 'gm1'], 'Spanish': ['bi4', 'bj2', 'cv7', 'dc6', 'df4', 'dk0', 'eo8', 'fa2', 'fx7', 'fy1', 'gq4', 'gu1']}\n"
     ]
    }
   ],
   "source": [
    "llang_dict_45={}\n",
    "for i in in45_loose:\n",
    "    df_ind=binarySearchArr(all_anon_loose,i)\n",
    "    key=sorted_loose.at[df_ind,'L1']\n",
    "    val=sorted_loose.at[df_ind,'anon_id']\n",
    "    \n",
    "    if key in llang_dict_45:\n",
    "        llang_dict_45[key].append(val)\n",
    "    else:\n",
    "        llang_dict_45[key]=[val]\n",
    "    \n",
    "print(llang_dict_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-5:\n",
      "Korean 2\n",
      "Chinese 4\n",
      "Spanish 2\n"
     ]
    }
   ],
   "source": [
    "print(\"3-5:\")\n",
    "\n",
    "for key, value in llang_dict_35.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-4:\n",
      "Korean 14\n",
      "Chinese 9\n",
      "Spanish 5\n"
     ]
    }
   ],
   "source": [
    "print(\"3-4:\")\n",
    "\n",
    "for key, value in llang_dict_34.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-5:\n",
      "Korean 24\n",
      "Chinese 25\n",
      "Spanish 12\n"
     ]
    }
   ],
   "source": [
    "print(\"4-5:\")\n",
    "\n",
    "for key, value in llang_dict_45.items():\n",
    "    print(key,len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer counts for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_counts=sorted_sac.copy()\n",
    "e_counts[\"Number of Questions\"]=e_counts.anon_id.count()\n",
    "entire_counts=e_counts.groupby(['anon_id','native_language'])['Number of Questions'].count()\n",
    "qcount_entire=entire_counts.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>native_language</th>\n",
       "      <th>Number of Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa1</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa3</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>ha0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ha6</td>\n",
       "      <td>Korean</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>hb4</td>\n",
       "      <td>Korean</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>hc1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id native_language  Number of Questions\n",
       "0       aa0         Spanish                   17\n",
       "1       aa1         Chinese                   22\n",
       "2       aa3         Chinese                    8\n",
       "3       aa8          Korean                   34\n",
       "4       aa9          Korean                   27\n",
       "..      ...             ...                  ...\n",
       "470     ha0          Korean                    3\n",
       "471     ha2          Korean                   44\n",
       "472     ha6          Korean                   32\n",
       "473     hb4          Korean                   40\n",
       "474     hc1          Korean                    5\n",
       "\n",
       "[475 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qcount_entire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in all 3\n",
      "Spanish counts:  108 \n",
      "Chinese counts:  460 \n",
      "Korean counts:  795\n"
     ]
    }
   ],
   "source": [
    "espanish,echinese,ekorean=find_counts(qcount_entire,all_three,'native_language')\n",
    "print('Counts in all 3\\nSpanish counts: ',espanish,'\\nChinese counts: ',echinese,'\\nKorean counts: ',ekorean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 3 and 4\n",
      "Spanish counts:  157 \n",
      "Chinese counts:  907 \n",
      "Korean counts:  1552\n"
     ]
    }
   ],
   "source": [
    "espanish34,echinese34,ekorean34=find_counts(qcount_entire,lvls34,'native_language')\n",
    "print('Counts in 3 and 4\\nSpanish counts: ',espanish34,'\\nChinese counts: ',echinese34,'\\nKorean counts: ',ekorean34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 4 and 5\n",
      "Spanish counts:  298 \n",
      "Chinese counts:  1701 \n",
      "Korean counts:  1974\n"
     ]
    }
   ],
   "source": [
    "espanish45,echinese45,ekorean45=find_counts(qcount_entire,lvls45,'native_language')\n",
    "print('Counts in 4 and 5\\nSpanish counts: ',espanish45,'\\nChinese counts: ',echinese45,'\\nKorean counts: ',ekorean45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Counts for Loose dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>Number of Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa3</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ab6</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>gz2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>gz5</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>gz7</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ha0</td>\n",
       "      <td>Korean</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    anon_id       L1  Number of Questions\n",
       "0       aa0  Spanish                    8\n",
       "1       aa3  Chinese                    2\n",
       "2       aa8   Korean                   13\n",
       "3       aa9   Korean                   18\n",
       "4       ab6  Chinese                    2\n",
       "..      ...      ...                  ...\n",
       "278     gz2   Korean                    8\n",
       "279     gz5  Chinese                    6\n",
       "280     gz7   Korean                    5\n",
       "281     ha0   Korean                    3\n",
       "282     ha2   Korean                   19\n",
       "\n",
       "[283 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_counts=textcount_loose.copy()\n",
    "l_counts[\"Number of Questions\"]=l_counts.anon_id.count()\n",
    "loose_counts=l_counts.groupby(['anon_id','L1'])['Number of Questions'].count()\n",
    "qcount_loose=loose_counts.reset_index()\n",
    "qcount_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in all three\n",
      "Spanish counts:  23 \n",
      "Chinese counts:  34 \n",
      "Korean counts:  26\n"
     ]
    }
   ],
   "source": [
    "lspanish,lchinese,lkorean=find_counts(qcount_loose, all_three_loose,'L1')\n",
    "print('Counts in all three\\nSpanish counts: ',lspanish,'\\nChinese counts: ',lchinese,'\\nKorean counts: ',lkorean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 3 and 4\n",
      "Spanish counts:  44 \n",
      "Chinese counts:  64 \n",
      "Korean counts:  146\n"
     ]
    }
   ],
   "source": [
    "lspanish34,lchinese34,lkorean34=find_counts(qcount_loose,in34_loose,'L1')\n",
    "print('Counts in 3 and 4\\nSpanish counts: ',lspanish34,'\\nChinese counts: ',lchinese34,'\\nKorean counts: ',lkorean34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts in 4 and 5\n",
      "Spanish counts:  128 \n",
      "Chinese counts:  305 \n",
      "Korean counts:  289\n"
     ]
    }
   ],
   "source": [
    "lspanish45,lchinese45,lkorean45=find_counts(qcount_loose, in45_loose,'L1')\n",
    "print('Counts in 4 and 5\\nSpanish counts: ',lspanish45,'\\nChinese counts: ',lchinese45,'\\nKorean counts: ',lkorean45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokinizing Essays for Loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>level_id</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>wordtype_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>Barber, chef by profession, but an expert on a...</td>\n",
       "      <td>barber chef by profession but an expert on agr...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>The article \"English as Co star\" support the f...</td>\n",
       "      <td>the article english as co star support the fac...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>In this article the authors Goleman, Kaufman a...</td>\n",
       "      <td>in this article the authors goleman kaufman an...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>Flow in the sense expressed in the text \"The C...</td>\n",
       "      <td>flow in the sense expressed in the text the cr...</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Bill Gates, in this conference, explained two ...</td>\n",
       "      <td>bill gates in this conference explained two bi...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>In Korea, a lot of parents who want their chil...</td>\n",
       "      <td>in korea a lot of parents who want their child...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>4.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>The importance of women is getting greater in ...</td>\n",
       "      <td>the importance of women is getting greater in ...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>4.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>A natural disaster is the effect of the natura...</td>\n",
       "      <td>a natural disaster is the effect of the natura...</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>Fresh air, the npr's program, invited Ben Stil...</td>\n",
       "      <td>fresh air the nprs program invited ben stiller...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>ha2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>5.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>I think human language and nonhuman primate la...</td>\n",
       "      <td>i think human language and nonhuman primate la...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     anon_id       L1  level_id  text3_len  \\\n",
       "0        aa0  Spanish       5.0      109.0   \n",
       "1        aa0  Spanish       5.0      190.0   \n",
       "2        aa0  Spanish       5.0      193.0   \n",
       "3        aa0  Spanish       5.0      170.0   \n",
       "4        aa0  Spanish       5.0       85.0   \n",
       "...      ...      ...       ...        ...   \n",
       "1792     ha2   Korean       5.0      177.0   \n",
       "1791     ha2   Korean       4.0       69.0   \n",
       "1808     ha2   Korean       4.0      283.0   \n",
       "1799     ha2   Korean       5.0      315.0   \n",
       "1809     ha2   Korean       5.0      168.0   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  \\\n",
       "0     Barber, chef by profession, but an expert on a...   \n",
       "1     The article \"English as Co star\" support the f...   \n",
       "2     In this article the authors Goleman, Kaufman a...   \n",
       "3     Flow in the sense expressed in the text \"The C...   \n",
       "4     Bill Gates, in this conference, explained two ...   \n",
       "...                                                 ...   \n",
       "1792  In Korea, a lot of parents who want their chil...   \n",
       "1791  The importance of women is getting greater in ...   \n",
       "1808  A natural disaster is the effect of the natura...   \n",
       "1799  Fresh air, the npr's program, invited Ben Stil...   \n",
       "1809  I think human language and nonhuman primate la...   \n",
       "\n",
       "                                         processed_text wordtype_len  \n",
       "0     barber chef by profession but an expert on agr...           79  \n",
       "1     the article english as co star support the fac...          117  \n",
       "2     in this article the authors goleman kaufman an...          122  \n",
       "3     flow in the sense expressed in the text the cr...          103  \n",
       "4     bill gates in this conference explained two bi...           66  \n",
       "...                                                 ...          ...  \n",
       "1792  in korea a lot of parents who want their child...          100  \n",
       "1791  the importance of women is getting greater in ...           53  \n",
       "1808  a natural disaster is the effect of the natura...          135  \n",
       "1799  fresh air the nprs program invited ben stiller...          159  \n",
       "1809  i think human language and nonhuman primate la...          100  \n",
       "\n",
       "[1810 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsort_loose=textcount_loose.sort_values(by=['anon_id'])\n",
    "textsort_loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three\n",
      "Spanish Token Average:  132.8695652173913 \n",
      "Korean Token Average:  181.3846153846154 \n",
      "Chinese Token Average: 188.64705882352942\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three')\n",
    "finding_tokens(loose,all_three_loose,'L1','text3_len',qcount_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4\n",
      "Spanish Token Average:  140.8181818181818 \n",
      "Korean Token Average:  152.14383561643837 \n",
      "Chinese Token Average: 169.953125\n"
     ]
    }
   ],
   "source": [
    "print('Loose 3-4')\n",
    "finding_tokens(loose,in34_loose,'L1','text3_len',qcount_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5\n",
      "Spanish Token Average:  203.5 \n",
      "Korean Token Average:  231.42560553633217 \n",
      "Chinese Token Average: 238.32131147540983\n"
     ]
    }
   ],
   "source": [
    "print('Loose 4-5')\n",
    "finding_tokens(loose,in45_loose,'L1','text3_len',qcount_loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three, Level 3\n",
      "Spanish Average:  101.8\n",
      "Korean Average:  134.5\n",
      "Chinese Average:  131.8181818181818\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three, Level 3')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',all_three_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three, Level 4\n",
      "Spanish Average:  172.5\n",
      "Korean Average:  191.5\n",
      "Chinese Average:  264.14285714285717\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three, Level 4')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',all_three_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose All Three, Level 5\n",
      "Spanish Average:  104.33333333333333\n",
      "Korean Average:  211.0\n",
      "Chinese Average:  194.6875\n"
     ]
    }
   ],
   "source": [
    "print('Loose All Three, Level 5')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',all_three_loose,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 3\n",
      "Spanish Average:  108.8\n",
      "Korean Average:  121.32142857142857\n",
      "Chinese Average:  138.3\n"
     ]
    }
   ],
   "source": [
    "print('Loose 3-4, Level 3')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in34_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 4\n",
      "Spanish Average:  176.52380952380952\n",
      "Korean Average:  169.46511627906978\n",
      "Chinese Average:  200.66666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Loose 3-4, Level 4')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in34_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 4\n",
      "Spanish Average:  214.77272727272728\n",
      "Korean Average:  186.7709497206704\n",
      "Chinese Average:  208.28947368421052\n"
     ]
    }
   ],
   "source": [
    "print('Loose 4-5, Level 4')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in45_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 5\n",
      "Spanish Average:  208.73076923076923\n",
      "Korean Average:  313.77884615384613\n",
      "Chinese Average:  278.6830985915493\n"
     ]
    }
   ],
   "source": [
    "print('Loose 4-5, Level 5')\n",
    "tokenAvg(textsort_loose,'text3_len','L1',in45_loose,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Essays for Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_sac=stu_ans_crs.sort_values(by=['anon_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three\n",
      "Spanish Token Average:  182.3148148148148 \n",
      "Korean Token Average:  210.0867924528302 \n",
      "Chinese Token Average: 211.56521739130434\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three')\n",
    "finding_tokens(stu_ans_crs,all_three,'native_language','text_len',qcount_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4\n",
      "Spanish Token Average:  187.2675159235669 \n",
      "Korean Token Average:  187.3137886597938 \n",
      "Chinese Token Average: 195.03528114663726\n"
     ]
    }
   ],
   "source": [
    "print('Whole 3-4')\n",
    "finding_tokens(stu_ans_crs,lvls34,'native_language','text_len',qcount_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5\n",
      "Spanish Token Average:  219.95302013422818 \n",
      "Korean Token Average:  236.97821681864235 \n",
      "Chinese Token Average: 247.12580834803057\n"
     ]
    }
   ],
   "source": [
    "print('Whole 4-5')\n",
    "finding_tokens(stu_ans_crs,lvls45,'native_language','text_len',qcount_entire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three, Level 3\n",
      "Spanish Average:  105.19444444444444\n",
      "Korean Average:  163.57309941520467\n",
      "Chinese Average:  143.54464285714286\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three, Level 3')\n",
    "tokenAvg(sort_sac,'text_len','native_language',all_three,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three, Level 4\n",
      "Spanish Average:  249.21153846153845\n",
      "Korean Average:  232.37891737891738\n",
      "Chinese Average:  207.9468085106383\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three, Level 4')\n",
    "tokenAvg(sort_sac,'text_len','native_language',all_three,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole All Three, Level 5\n",
      "Spanish Average:  147.2\n",
      "Korean Average:  210.56043956043956\n",
      "Chinese Average:  267.2420382165605\n"
     ]
    }
   ],
   "source": [
    "print('Whole All Three, Level 5')\n",
    "tokenAvg(sort_sac,'text_len','native_language',all_three,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 3\n",
      "Spanish Average:  115.32835820895522\n",
      "Korean Average:  141.3293172690763\n",
      "Chinese Average:  144.34782608695653\n"
     ]
    }
   ],
   "source": [
    "print('Whole 3-4, Level 3')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls34,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 4\n",
      "Spanish Average:  267.57142857142856\n",
      "Korean Average:  211.13202614379085\n",
      "Chinese Average:  207.68941176470588\n"
     ]
    }
   ],
   "source": [
    "print('Whole 3-4, Level 4')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls34,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 4\n",
      "Spanish Average:  244.72027972027973\n",
      "Korean Average:  224.41098677517803\n",
      "Chinese Average:  225.59795134443021\n"
     ]
    }
   ],
   "source": [
    "print('Whole 4-5, Level 4')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls45,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 5\n",
      "Spanish Average:  224.9075630252101\n",
      "Korean Average:  267.35121951219514\n",
      "Chinese Average:  283.1055900621118\n"
     ]
    }
   ],
   "source": [
    "print('Whole 4-5, Level 5')\n",
    "tokenAvg(sort_sac,'text_len','native_language',lvls45,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Unique Word Counts for Loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5, Level 3\n",
      "Spanish Average:  59.6\n",
      "Korean Average:  74.16666666666667\n",
      "Chinese Average:  72.27272727272727\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5, Level 3\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',all_three_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5, Level 4\n",
      "Spanish Average:  92.7\n",
      "Korean Average:  96.875\n",
      "Chinese Average:  122.14285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5, Level 4\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',all_three_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5, Level 5\n",
      "Spanish Average:  67.66666666666667\n",
      "Korean Average:  116.5\n",
      "Chinese Average:  99.4375\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5, Level 5\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',all_three_loose,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 3\n",
      "Spanish Average:  62.5\n",
      "Korean Average:  69.05357142857143\n",
      "Chinese Average:  73.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4, Level 3\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in34_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4, Level 4\n",
      "Spanish Average:  92.71428571428571\n",
      "Korean Average:  90.70930232558139\n",
      "Chinese Average:  102.27777777777777\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4, Level 4\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in34_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 4\n",
      "Spanish Average:  110.63636363636364\n",
      "Korean Average:  98.26815642458101\n",
      "Chinese Average:  104.84210526315789\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5, Level 4\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in45_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5, Level 5\n",
      "Spanish Average:  110.61538461538461\n",
      "Korean Average:  144.01923076923077\n",
      "Chinese Average:  133.6549295774648\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5, Level 5\")\n",
    "tokenAvg(textcount_loose,'wordtype_len','L1',in45_loose,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Word Counts Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 3\n",
      "Spanish Average:  61.19444444444444\n",
      "Korean Average:  87.5906432748538\n",
      "Chinese Average:  80.59821428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 3\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',all_three,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 4\n",
      "Spanish Average:  117.61538461538461\n",
      "Korean Average:  113.98290598290598\n",
      "Chinese Average:  105.06914893617021\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 4\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',all_three,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 5\n",
      "Spanish Average:  85.6\n",
      "Korean Average:  103.13553113553114\n",
      "Chinese Average:  127.6624203821656\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 5\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',all_three,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 3\n",
      "Spanish Average:  64.94029850746269\n",
      "Korean Average:  78.77309236947791\n",
      "Chinese Average:  77.59937888198758\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 3\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls34,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 4\n",
      "Spanish Average:  123.14285714285714\n",
      "Korean Average:  106.1032679738562\n",
      "Chinese Average:  104.78117647058824\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 4\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls34,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 4\n",
      "Spanish Average:  119.08391608391608\n",
      "Korean Average:  111.34893184130213\n",
      "Chinese Average:  114.32266325224072\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 4\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls45,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 5\n",
      "Spanish Average:  114.3109243697479\n",
      "Korean Average:  124.68170731707318\n",
      "Chinese Average:  133.51925465838508\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 5\")\n",
    "tokenAvg(sort_sac,'wordtype_len','native_language',lvls45,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token/Type Ratio Loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5 Ratio, Level 3\n",
      "Spanish Ratio:  0.5854616895874264\n",
      "Korean Ratio:  0.5514250309789344\n",
      "Chinese Ratio:  0.5482758620689655\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5 Ratio, Level 3\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',all_three_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5 Ratio, Level 4\n",
      "Spanish Ratio:  0.5373913043478261\n",
      "Korean Ratio:  0.5058746736292428\n",
      "Chinese Ratio:  0.46241211465657106\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5 Ratio, Level 4\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',all_three_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-5 Ratio, Level 5\n",
      "Spanish Ratio:  0.6485623003194889\n",
      "Korean Ratio:  0.5521327014218009\n",
      "Chinese Ratio:  0.5107544141252006\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-5 Ratio, Level 5\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',all_three_loose,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4 Ratio, Level 3\n",
      "Spanish Ratio:  0.5744485294117647\n",
      "Korean Ratio:  0.5691786870768325\n",
      "Chinese Ratio:  0.5285610990600144\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4 Ratio, Level 3\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in34_loose,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 3-4 Ratio, Level 4\n",
      "Spanish Ratio:  0.5252225519287833\n",
      "Korean Ratio:  0.535268285988747\n",
      "Chinese Ratio:  0.5096899224806202\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 3-4 Ratio, Level 4\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in34_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5 Ratio, Level 4\n",
      "Spanish Ratio:  0.5151322751322751\n",
      "Korean Ratio:  0.5261426178511606\n",
      "Chinese Ratio:  0.503348073278585\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5 Ratio, Level 4\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in45_loose,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loose 4-5 Ratio, Level 5\n",
      "Spanish Ratio:  0.5299428782015847\n",
      "Korean Ratio:  0.45898323782673983\n",
      "Chinese Ratio:  0.47959467313572385\n"
     ]
    }
   ],
   "source": [
    "print(\"Loose 4-5 Ratio, Level 5\")\n",
    "ratio(textcount_loose,'text3_len','wordtype_len','L1',in45_loose,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type/Token Ratio Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 3\n",
      "Spanish Ratio:  0.581726960654872\n",
      "Korean Ratio:  0.5354831790068285\n",
      "Chinese Ratio:  0.5614853517447285\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 3\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',all_three,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 4\n",
      "Spanish Ratio:  0.4719499961416776\n",
      "Korean Ratio:  0.4905045056090234\n",
      "Chinese Ratio:  0.5052693507955185\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 4\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',all_three,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-5, Level 5\n",
      "Spanish Ratio:  0.5815217391304348\n",
      "Korean Ratio:  0.4898143799036237\n",
      "Chinese Ratio:  0.477703362966847\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-5, Level 5\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',all_three,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 3\n",
      "Spanish Ratio:  0.5630904620163064\n",
      "Korean Ratio:  0.5573726236821914\n",
      "Chinese Ratio:  0.5375860585197935\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 3\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls34,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 3-4, Level 4\n",
      "Spanish Ratio:  0.4602242391884677\n",
      "Korean Ratio:  0.5025446395403551\n",
      "Chinese Ratio:  0.5045089953323968\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 3-4, Level 4\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls34,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 4\n",
      "Spanish Ratio:  0.48661237319617084\n",
      "Korean Ratio:  0.49618306768935067\n",
      "Chinese Ratio:  0.5067539956411188\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 4\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls45,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole 4-5, Level 5\n",
      "Spanish Ratio:  0.508257360633687\n",
      "Korean Ratio:  0.46635922418669146\n",
      "Chinese Ratio:  0.471623519087319\n"
     ]
    }
   ],
   "source": [
    "print(\"Whole 4-5, Level 5\")\n",
    "ratio(sort_sac,'text_len','wordtype_len','native_language',lvls45,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Korean': ['an5', 'bv9'],\n",
       " 'Chinese': ['ar8', 'bl4', 'bp4', 'bz2'],\n",
       " 'Spanish': ['fa2', 'fy1']}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llang_dict_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3Langs</th>\n",
       "      <th>Item</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text1_len</th>\n",
       "      <th>text2_len</th>\n",
       "      <th>text3_len</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2 (line breaks/extra spaces removed, spaces added to reach 60)</th>\n",
       "      <th>text3 (edits made to fix word counts)</th>\n",
       "      <th>Judgement</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>C12</td>\n",
       "      <td>141.0</td>\n",
       "      <td>aj8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>\\n\\n\\n Today, I am going to describe one of my...</td>\n",
       "      <td>Today, I am going to describe one of my classm...</td>\n",
       "      <td>Today, I am going to describe one of my classm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>C13</td>\n",
       "      <td>143.0</td>\n",
       "      <td>az8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Female</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>My niece is 3 years old who is my younger brot...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>K9</td>\n",
       "      <td>133.0</td>\n",
       "      <td>az2</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>When I was in Germany, I met a friend who was ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>K11</td>\n",
       "      <td>135.0</td>\n",
       "      <td>at8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>my friend is ANON_NAME_0. she is a my ELI frie...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>K20</td>\n",
       "      <td>188.0</td>\n",
       "      <td>eh9</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Male</td>\n",
       "      <td>118.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>w</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>Younghun is my best friend. His facial appeara...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>Y</td>\n",
       "      <td>C3207</td>\n",
       "      <td>48246.0</td>\n",
       "      <td>bv8</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>g</td>\n",
       "      <td>6105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>The quote that talks about a true friend is so...</td>\n",
       "      <td>The quote that talks about a true friend is so...</td>\n",
       "      <td>The quote that talks about a true friend is so...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>Y</td>\n",
       "      <td>K3559</td>\n",
       "      <td>48254.0</td>\n",
       "      <td>dr8</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>g</td>\n",
       "      <td>6105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>I think this means that home is not just a pla...</td>\n",
       "      <td>I think this means that home is not just a pla...</td>\n",
       "      <td>I think this means that home is not just a pla...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>Y</td>\n",
       "      <td>S781</td>\n",
       "      <td>48252.0</td>\n",
       "      <td>bl3</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Female</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>g</td>\n",
       "      <td>6105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>This quotation means that the majority of peop...</td>\n",
       "      <td>This quotation means that the majority of peop...</td>\n",
       "      <td>This quotation means that the majority of peop...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1812 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3Langs   Item  answer_id anon_id       L1   gender  course_id  level_id  \\\n",
       "0         Y    C12      141.0     aj8  Chinese     Male      118.0       5.0   \n",
       "1         Y    C13      143.0     az8  Chinese   Female      118.0       5.0   \n",
       "2         Y     K9      133.0     az2   Korean     Male      118.0       5.0   \n",
       "3         Y    K11      135.0     at8   Korean   Female      118.0       5.0   \n",
       "4         Y    K20      188.0     eh9   Korean     Male      118.0       5.0   \n",
       "...     ...    ...        ...     ...      ...      ...        ...       ...   \n",
       "1807      Y  C3207    48246.0     bv8  Chinese  Unknown     1043.0       4.0   \n",
       "1808      Y  K3559    48254.0     dr8   Korean   Female     1043.0       4.0   \n",
       "1809      Y   S781    48252.0     bl3  Spanish   Female     1043.0       4.0   \n",
       "1810    NaN    NaN        NaN     NaN      NaN      NaN        NaN       NaN   \n",
       "1811    NaN    NaN        NaN     NaN      NaN      NaN        NaN       NaN   \n",
       "\n",
       "     class_id  question_id  version  text1_len  text2_len  text3_len  \\\n",
       "0           w         17.0      1.0      121.0      114.0      114.0   \n",
       "1           w         17.0      1.0       96.0       95.0       95.0   \n",
       "2           w         17.0      1.0      130.0      128.0      128.0   \n",
       "3           w         17.0      3.0      104.0      105.0      105.0   \n",
       "4           w         17.0      1.0       98.0       97.0       97.0   \n",
       "...       ...          ...      ...        ...        ...        ...   \n",
       "1807        g       6105.0      1.0      133.0      131.0      131.0   \n",
       "1808        g       6105.0      1.0      175.0      162.0      162.0   \n",
       "1809        g       6105.0      1.0      157.0      146.0      146.0   \n",
       "1810      NaN          NaN      NaN        NaN        NaN        NaN   \n",
       "1811      NaN          NaN      NaN        NaN        NaN        NaN   \n",
       "\n",
       "                                                  text1  \\\n",
       "0     \\n\\n\\n Today, I am going to describe one of my...   \n",
       "1     My niece is 3 years old who is my younger brot...   \n",
       "2     When I was in Germany, I met a friend who was ...   \n",
       "3     my friend is ANON_NAME_0. she is a my ELI frie...   \n",
       "4     Younghun is my best friend. His facial appeara...   \n",
       "...                                                 ...   \n",
       "1807  The quote that talks about a true friend is so...   \n",
       "1808  I think this means that home is not just a pla...   \n",
       "1809  This quotation means that the majority of peop...   \n",
       "1810                                                NaN   \n",
       "1811                                                NaN   \n",
       "\n",
       "     text2 (line breaks/extra spaces removed, spaces added to reach 60)  \\\n",
       "0     Today, I am going to describe one of my classm...                   \n",
       "1     My niece is 3 years old who is my younger brot...                   \n",
       "2     When I was in Germany, I met a friend who was ...                   \n",
       "3     my friend is ANON_NAME_0. she is a my ELI frie...                   \n",
       "4     Younghun is my best friend. His facial appeara...                   \n",
       "...                                                 ...                   \n",
       "1807  The quote that talks about a true friend is so...                   \n",
       "1808  I think this means that home is not just a pla...                   \n",
       "1809  This quotation means that the majority of peop...                   \n",
       "1810                                                NaN                   \n",
       "1811                                                NaN                   \n",
       "\n",
       "                  text3 (edits made to fix word counts)  Judgement Notes  \n",
       "0     Today, I am going to describe one of my classm...        1.0   NaN  \n",
       "1     My niece is 3 years old who is my younger brot...        1.0   NaN  \n",
       "2     When I was in Germany, I met a friend who was ...        1.0   NaN  \n",
       "3     my friend is ANON_NAME_0. she is a my ELI frie...        1.0   NaN  \n",
       "4     Younghun is my best friend. His facial appeara...        1.0   NaN  \n",
       "...                                                 ...        ...   ...  \n",
       "1807  The quote that talks about a true friend is so...        3.0   NaN  \n",
       "1808  I think this means that home is not just a pla...        3.0   NaN  \n",
       "1809  This quotation means that the majority of peop...        3.0   NaN  \n",
       "1810                                                NaN        NaN   NaN  \n",
       "1811                                                NaN        NaN   NaN  \n",
       "\n",
       "[1812 rows x 19 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAns(idlist, df, lvl, txtcol):\n",
    "    idlist.sort()\n",
    "    index=0\n",
    " #   for name in idlist:\n",
    "    for anon in df.anon_id:\n",
    "        if anon in idlist:\n",
    "            if df.at[index,'level_id']==lvl:\n",
    "                print(anon,\":\" , df.at[index,txtcol],'\\n')\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bl4 : I felt frustrated when I talk to native speaker on the phone. I always kept on asking the person to repeat the question. Sometimes, I can figure out every easily but most of time I really want to give up. I always try to talk myself into an insistence. It is a good opportunity to practice in English so that I told myself never hang up the phone before someone doing that. \n",
      "\n",
      "bz2 : One scene can give people different feelings when it is showed from the different ways. And some people like to record the scene by a camera while other people like to record by a painting. But what the difference between a scene from a camera and from a painting? The difference between a photo from camera and a painting is that a photo is truer than a painting. Because a camera can catch more derails which are easily missed, otherwise the details provide more information about the scene. Also, a picture is more lively than a painting. When people take a picture, it just takes a few seconds, and everything will recorded naturally. However, a painting takes a long time to finish. Another difference is that a camera is easy to carry, but the equipments of a painting are more complicated. Although there are so much difference between a photo and painting. They still ca replace each other, because they have many similarities. First, both photos and paintings show beautiful feeling to people. Second, both a photo and a painting show what he author wants people to know. People can realize the meaning of a photo or a painting. Third, both taking photos and drawing paintings cost a lot of money. Artists have to spend a lot of money to but equipments and materials to create. Even though there are a lot of differences between a photo and painting, they show the same thing. Both of them bring people beautiful scenes and let people know more about the beautiful things in this world. \n",
      "\n",
      "bz2 : Stress comes from everywhere. Not only adults feel it, but also children. Dr Gautamadas quoted a recent survey, and he said \"The perception of stress increases with age, along with the urge to perform. And stress has a negative impact on performance. So it is a vicious circle.\" Adults feel stress because of the work, family. Children or teenagers feel stress because of school homework, lots of tests or parents ask high grades. But how to release the stress and help relax. Here I have some ways. First, watch a comedy. When I feel stress, I sometimes watch a comedy. With watching it, I laugh loudly. Meanwhile I temporary forget the stress and the things which bothered me. After I watch a comedy I feel much better. Although I still have solved the problem, I feel I have energy to fight these problem and stress. Second, sometimes I go to a beach. I love the blue sea and the sky. When I see the sky and the sea, I feel very comfortable and it just like takes away most of my stress. I always walking at on the beach to help me relaxing, too. The thing I do but not usually at the beach is shout loudly. Shout at the beach is relaxing, because it is very spacious and won't bother any person. Last one, I think is the most useful one. Talk to my family or friends. When I talk to my family and friends or spend time together, they always provide some solutions or tell me what is the problem. After the talking, I also feel relax. This is the way I always choose to release my stress. There are still more other way to release stress. Through these ways you can feel much better about the stress, and have more energy to do everything. It is not so difficult. So don't feel upset the stress come. Just choose the way you like best, and fight the stress!  \n",
      "\n",
      "ar8 : Can you imagine 1.3 billion people celebrating at the same time? It happens only during the Spring Festival in China. The Spring Festival is the most important traditional Chinese holiday. It usually begins in late January or early February. It depends on which day is the first day of the lunar year in the Chinese calendar, so the Spring Festival is also the Chinese New Year. During the Spring Festival, all family members come together, prepare a lot of food, including a big New Year's Eve dinner, and celebrate for over a week. People who are away from home for work or school go home before the Spring Festival. About half a month before the holiday is the busiest time for travel. Airports, railway stations and long-distance bus stations are crowded with people heading home. However busy people are or however far they live from home, they will go back to visit parents and relatives. Sometimes, this is the only chance in the year for some hard-workers or students to go back home. This is the reason why the Spring Festival is so important for the Chinese people. In China, food is not just important but a part of culture. It is traditional to eat certain food during the holiday, such as dumplings, nian-gao, fish and spring rolls. These have individual meanings for good fortune. For example, my favorite food is nian-gao. The word \"nian\" means year in Chinese, and the word \"gao\" means high in Chinese, so nian-gao sounds like \"getting higher year by year\". Chinese people always like to save more money at the end of the year, so they like to prepare fish as a main dish because the Chinese pronunciation of fish sounds like \"more saving\". Most people like spring-roll very much because it contains pork and vegetables. They are very nutritious and delicious. The big New Year's Eve dinner is the most important in a year, so it must be abundant and there is a great variety of different dishes. There are a lot of activities during the Spring Festival, and people usually start to prepare for the celebration of the Festival a week before. They decorate their houses with red lanterns and red paper-cuttings. They buy a lot of food. On New Year's Eve, families get together at the grandparents' house usually at noon. Some of them start to prepare the big dinner, and the other people chat, watch TV, and play card games. People usually go out to see fireworks after dinner and again at midnight. Many people set off fireworks on the streets. With the sound and light of fireworks, you can absolutely feel the atmosphere of the Spring Festival. This is my favorite part of Spring Festival. The most important holiday in China is a great time for people to get together and celebrate. There are a lot of food and activities, and it's always an exciting and happy moment for 1.3 billion people. \n",
      "\n",
      "ar8 : Can you define the meaning of happiness? I feel it's hard to explain. In my opinion, happiness is an abstract thing, because it's a feeling and a subjective sensation. Sometimes people feel happy easily, but sometimes they try very hard to search for happiness. I think to be happy we could try to do the following: we should actively discover happiness, cherish the things we already have, and share the happiness with other people. I believe happiness exists around us all the time, but we should actively discover it. You may face some difficulties, which will make you feel disappointed. However, you can always learn from the difficulties and even failures to get valuable experiences. For example, when I first started to learn classic music, I was told that my body was not strong enough to sing as a soprano. I was sad first, but I decided to face this problem and started to take many physical exercises. The exercises were very hard, but I felt really happy that I could improve every day. After rigorous training, I successfully passed the auditions of music schools. I was so happy that the person told me my weakness, so that I could face it and correct it. This experience taught me that we should always actively find happiness even under difficult situations, and sometimes overcoming a difficulty could bring tremendous happiness. People always pursue the things they don't have, and overlook the things they already have. Sometimes people complain why other people feel happy but I don't. You should ask yourself some questions. Is there somebody who often calls you and worries about you? Do you have some family members who always love you and take care of you? Sometimes you become used to the caring from your family and friends, and then you take it for granted. If you appreciate their concern, you will realize how lucky you are, and it will make you feel happy. Sometimes people around you can share and even enhance your experience of happiness. For example, if you achieve something, and your family and friends can share the excitement and joy with you, you will feel much happier. An athlete who just won an Olympic gold medal would feel extremely happy, because it is not only a personal accomplishment, but also an achievement shared by his whole nation. So it is important for us to share our happiness, because it not only makes other people happy, it can also make ourselves feel happier. Each individual feels happy for different reasons. But as long as we actively discover happiness, cherish what we already have and share the happiness with other people, we will have a happy life.  \n",
      "\n",
      "bp4 : My new room is bright, warm and fragrant. This cheers me up, because it make me feel I am in my home. Sometime I feel longly in my room, I try to figure out what is missing? At last, I find out what's missing. It's my family's pictures. I pick out some picture from my family. Then I put them up on the wall. Every time I saw these pictures I missed my family, so I always called back to my family. Now I love my room very much, because everything was setting down. \n",
      "\n",
      "bp4 : A best friend is who might be another you. Maybe, but I don't think so. I have a friend who have totally different personality from mine. I'm very easy-going, but she is very shy. I can meet new friends in short time. Even thought she is shy, but she have some personality I don't have. Sometime I felt sad. Ling, who always encourage me all the time. Ling, who always care about me and my felling. I'm so glad I have this friend with me. I can't image I don't have her in my life. Even thought we don't have similar personality that we still can be best friends. Ling, who have different personality with me, but we can cover each other. Now, we are in different country. We still connect with each other. Ling, who always encourage me, even thought We don't stay with each other. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lids35=['ar8', 'bl4', 'bp4', 'bz2']\n",
    "\n",
    "printAns(lids35, loose, 4, 'text3 (edits made to fix word counts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bl4 : when I was little, I always go home late after school because I like painting on the ground and do not noticed that times go by. During that time, the only thing I do is focus on my paint. Because of this reason, I do not limit by art teacher so that I am more created than in the class. \n",
      "\n",
      "bl4 : I was not good at study in middle school especially in math. I was interested in art but that is not the main subject for entrance examination. Due to this reason, my parents force me to give up learning painting and pay attention to study hard. Although I do not like this decision but I have no choice. After I pass the exam, I try to restart to painting but I think I already lose a lot of passion that times go by. If my parents supported me to painting, maybe I am a good painter right now. \n",
      "\n",
      "bz2 : In my country, Taiwan, has a lot of mythes and legends. One of my favorite mythes is the story of ANON_NAME_0 Tian-Ding. He was considered to be a hero in early 20th at Taipei city. At that time, Taiwan was governed by Japan in 50 years. During that 50 years, Taiwanese was discriminated against Japanese. There was a very big gate between rich people and poor people. ANON_NAME_0 Tian-Ding was a hero to help poor people, but what he had done was alleged to be a criminal my police. He stole rich people's money to help poor people. It is said not a right way to be a thief, but at that time poor people were squeezed by rich people, and they didn't have good live, they even were not able to eat food. Although government knew ANON_NAME_0 Tian-Ding was doing bad things, the government couldn't catch him, because ANON_NAME_0 Tian-Ding knew Kung Fu and he also knew how to change his look. He did help poor people that time. It is believe that ANON_NAME_0 ANON_NAME_1-Ding was betrayed, so the government got a chance to catch him. One day he was surrounded my many police officers and he jumped into a river, but a police shot him. Although we was injured, he kept swimming. Few days later, his cope was found in a cave. He was only 27 when he died. After that, people built a temple to remember him. He was a thief, but he was thought to be a super hero. He was not just help poor people, he also against Japanese government for Taiwanese. \n",
      "\n",
      "bz2 : Every one likes vacation, either short vacation or long vacation. Students always have long summer vacation. Once you have a long vacation, you won't want just stay at home. Since we have a long winter time, we want some warm sunshine. Summer vacation is a very good chance to get beautiful sunshine. When we have a long vacation, we have a lot of time to do what we want to do. However how to plane a vacation is an important thing. No one wants to west summer vacation, everyone wants to do something special during the time. Because of this, we should plane our vacation well, and we won't be regret at the end of the vacation. How to plane a vacation, make sure how long is the vacation, list what you want to do and make a budget. First of all, you need to check how long is your vacation. You don't want to run our of time, or come back to school late. Some people don't make sure how long is vacation, so they miss the first day of the class and some people just come back just one day before the school begin. Both of these two situations are not good, because you haven't ready to focus on study yet, you probably miss something at the beginning of the classes. In order to prevent this, we should make sure how many time do we have. Second, since we have a long vacation, we may want to do something special, such as have a trip in our own country or outside our own country, learning something new or have a part time job. Write down everything you want to do. Although we may not able to do everything, we can use this list and depend on our time to plane what we can do, what we can't do. Also with the list, we won't forget what we want to do. Third, make a budget. It is very important if you want to have a trip. We should make a budget, so we won't run out of money. We don't want to spend too much money during the vacation. We have to make sure if we have enough money to have a trip. As a student, all of our support is from our parents, it is not that mature to ask parents to support you have a trip! If we want a trip, we should save money before we do it. I love to take a long vacation, and during a long vacation, I always want to do a lot of tings. So I always make a list to for what I want to do, and try my best to make them come true. Check my time, make a list and make budget are my steps to plane my vacation. I hope these three steps can help people to plane a vacation! \n",
      "\n",
      "bz2 : A movie I have seen recently is BOLT. This movie is an animation which I like a lot. Bolt is a dog which has a lot of special power. He is also considered as a hero. However, he is just an normal dog. All super powers are not from him but from the people. Bolt thinks he is a super hero, so he needs to protect his owner whose name is Penny. Bolt never lives as an normal dogs. One day, he is sent to New York by accidentally. He meets a cat which is a normal cat. The cat led him go back to Hollywood. During the trip, he learns how to be a normal dog. He also realizes he is just a normal dog without any super power. Finally, he finds his owner and Penny quits the job. After they quit, they have their normal live. \n",
      "\n",
      "bz2 : Do you like to live in your hometown or live in the city which you are living now? To live in another country was my dream; I wanted to experience other citys' customs. It doesn't matter weather the city is in my native country or not. Now, my dream has come true; the city I am living now is Pittsburgh. I am here for studying English and I'll study for my bachelor also. I have been here 5 months, although I have adapted my new live, I still miss my home town sometimes. I compare the difference between my hometown and Pittsburgh: the weather, the food and the environment. I am from Taiwan, and my hometown is called Tainan. The weather there is always hot, even in winter it is not as cold as Pittsburgh's weather; it is warmer than Pittsburgh's. When I was in Tainan I never saw snow before, but after I came here I saw a lot of snow. I was so happy at first, however I am tired about seeing snow now. Also in Tainan, it only rain about two months in summer, unlike Pittsburgh, it's also rain in winter. When I came here, I ate much American food than I did before. I noticed that people in here are much salad, pizzas and hamburgs. Also I found that the American food is saltier than Taiwan's. The most different is that it is more convenient in my hometown than in Pittsburgh. I can go out to buy any food whenever I want, on the contrary, I can't do the same thing in Pittsburgh. There is another different, the environment. Most houses in Tainan were made by concrete, on the other hand, houses here are made by either wood or limestone which have stable framework. In my hometown, there are a lot of historic spots, but in Pittsburgh here has many museums instead. I do like Pittsburgh, the people are friendly, and here has a lof of museum, such as art museum or history museum. Not only museum, Pittsburgh has high quality of art. Although the weather here in winter is too cold for me, but I have chance to see snow. I like to travel, and experience diverse customs in different city, even though I feel it is good for me to stay in my hometown. However I never regret that I choose Pittsburgh as my second hometown. \n",
      "\n",
      "ar8 : Penny met her baby Koko 13 years ago. The Koko was a big female gorilla, and she was silly and playful. Penny and Koko shared common language and exchanged emotion each other. Doctor H. Cohn has researched Koko language for 13 years. He purposed Koko learn language suing sigh language. Koko learned to use sigh language expressing \"drink\" and \"eat\", then more and more. Since 1976, Penny and Koko have lived in the wild. Koko learned how to use sigh language to express the complex emotion like respect, trust and love. Koko liked to read books and look pictures from the books. She also liked to hear stories that penny read to her. She's beloved kid was a cat. After the cat was killed, she expressed the strong sadness through the sigh language. \n",
      "\n",
      "ar8 : China is the most populous state in the world with over 1.3 billion people. China is made up of 34 provinces, and the capital is Beijing. China is located in East Asia and bordered with Mongolia and Russia in the North; Vietnam, Laos and Burma in the South. The mainland China's eastern seaboard is bounded on the southeast by the South China Sea and on the east by the East China Sea beyond which lies Taiwan, Korea, and Japan. China is a country with diverse geography, culture and people. Two major rivers in China are the Yellow River and the Yangtze River. Most of China's arable lands lie along these rivers, and they were the centers of China's major ancient civilizations. There are also desserts and inhabitable mountains in the western part of China. In China, there are hundreds of dialects in different areas. The official language is mandarin. In addition to the majority Han people, there are 55 different minority ethnic groups in China. In conclusion, China is an amazing country with beautiful landscapes and rich culture. \n",
      "\n",
      "ar8 : The difference of language between non-human primate and human is that non-human primate cannot speak, and they cannot create a language by themselves. However, non-human primate has the ability of understanding and learning like humans. The gorilla Koko started learning sign-language after she was born in 1971. Through Koko's ability of understanding and imitation, she is able to use and understand more than 1,000 signs. She can also understand thousands of words of spoken English. Koko demonstrates that non-human primate has the capability to learn how to express their minds and emotions using sign-language. Furthermore, Koko can express her emotion through painting. Some of her paintings express an object like a cat or a bird, and some of her paintings express the emotions like anger or happiness. Through these examples, we can see non-human primate has the communication capabilities similar to humans'; namely, they can express their minds and emotions clearly and communicate with other humans. \n",
      "\n",
      "ar8 : This is a story about some surgery interns. Meredith, Cristina, George and Izzie are in the same group. Their supervising doctor is very strict, and gives them many rules. All of them met difficulties at first. For example, George got the chance to perform an appendectomy, but he didn't complete. Cristina really wanted to do an operation and almost got the chance. She and Meredith helped the attending doctor make the correct diagnosis for a patient in critical condition, but the doctor decided to give the chance to Meredith but not Cristina. Cristina was angry and sad. Fortunately, this situation didn't hurt their friendship. After the first 48-hour shift, they were exhausted, and they experienced complicated emotions. However, they reaffirmed their devotion to medicine. \n",
      "\n",
      "ar8 : My parents are very good example to show me how to be ideal parents. My father, who always brings me much joy and laugh, influences me deeply. He is very optimistic and funny. He is a very strict manager in his company, but he is never stringent to me because he thinks my mother is strict enough. He gives me a lot of freedom. My mother is a person who is very kind and graceful. She never has big problems with other people around her. Since I was very young, she has taught me to respect the elder, so that, when I eat something, I always ask my grandparents first if they want to eat. My mother teaches me many things that can help improve my personal quality, such as perseverance, courtesy, honesty and tolerance. She is not only my mother but also my best teacher. Anyway, in the future I want to be a mother like my mother and I want to find a husband whose personality is similar to my father's. \n",
      "\n",
      "ar8 : I don't have time to watch a movie recently. I only saw an episode of sitcom \"Grey's Anatomy\", recommended by my friend. Grey's Anatomy, of which I heard before, was popular in America. This was a story about some surgery interns. Meredith, Cristina, George and Izzie, who just left medical schools, met in the same group when they started their internship. Their supervising doctor, very strict, gave them many rules. All of them, excited first, faced a lot of difficulties. For example, George got a chance to perform an appendectomy, a minor operation, but he didn't complete it. Cristina, an Asian American, really wanted to do an operation and almost got the chance. She and Meredith helped an attending doctor make the correct diagnosis for a patient in critical condition. But the attending doctor, who had a one night stand with Meredith, decided to give the chance to Meredith instead of Cristina. Cristina was angry and sad. Fortunately, this situation didn't hurt their friendship. After the first 48-hour shift, the interns, having experienced complicated emotions, were exhausted. However, they reaffirmed their devotion to medicine. I was attracted, because this sitcom includes many thrilling and exciting stories, and provides exquisite details of the characters' personalities and emotions. I think I will keep watching this sitcom. \n",
      "\n",
      "ar8 : Author Nadine Cohodas has written a new biography of Nina Simone, called princess Noire. Nina Simone was a Julliard-trained pianist, a writer, a composer, and she became a voice of action and anger. She charts the life of a performer whose music was always an extension of- and a response to the world around her. According to Cohodas, Simone never had an interest in crossing the racial divide, and she wanted to confront it. Her song \"To Be Young, Gifted and Black\" became a staple of choirs in black schools across the country. Simone didn't see the racial progress, so she spent much of her life overseas. In addition to anger of racial issues, she alleged unfair compensation for her successful albums. Simone gave her last two performances at Carnegie Hall in the NYC. Cohodas says:\" it wasn't great, but everybody was so thrilled to see her. She became a goddess of culture. \n",
      "\n",
      "ar8 : The story talks about the history and the evolution of martini. It's made of gin and dry vermouth. It was invented in America and has become an icon. Martini was popular in urban areas, and it defined cool and modern. Its popularity faded in the late 1960s and 70s because of reduced expense account dining, the growth of health food stores and the appearance of other mind-altering substance. The classic cocktails made a comeback in the 1980s. New types of martinis with flavored vodka and fruit purees have become common in bars with loud music. According to the writer E. B. White, this is wrong because the drink should be an elixir of quietude. \n",
      "\n",
      "ar8 : In Leipzig, Germany, Sarie Teichfischer, who lost her job 2 years ago, founded a choir. Most of its members are unemployed. Some of them have been jobless for over five years. She started the choir because she realized that the unemployed could lose their confidence and found it difficult to interact with other people. By forming a group where everyone is experiencing the similar situation makes it much easier for people to socialize. The choir has indeed dramatically improved the life of the unemployed. Sarie Teichfischer noticed that the pride of the members is building up and the dignity is coming back. They also become more hopeful for the future and are actively looking for jobs. For Sarie Teichfischer, however, the only fear in getting a new job is that she would have to give up the choir. \n",
      "\n",
      "bp4 : Some people think Friday and Sunday is same for them, some people don't. For me, I think Friday is different from Sunday, even though Friday and Sunday is all called weekend. They still have something different, such as I still need to go to school or work on Friday, but I have a day off on Sunday. Friday, I have classes in the morning and afternoon, but I can stay outside or friends' houses late. I don't need to worry about can I wake up early in the morning. Also, I can feel relaxed to anticipate in all activities on Friday night. It's not neccesary to worry about next day because I don't have class on Saturday. Sunday, I don't have any classes, so I can do everything I want to do. I can hangout with my friends, clean my room, do my laundry, go shopping or do my homework...etc. I can do a lot of things on Sunday, however, I can't stay late at outside. Sunday's next day is Monday. I have classes on Monday. If I stay outside too late on Sunday, I can't wake up in the morning on Monday. I will attribute all of this on Sunday night. Friday and Sunday is also weekend, but they have some inherent different. I can go out at night on Friday, but I can't do this on Friday. I can have a totally day off on Sunday, but I don't have this on Friday. Are they same? I think yes, all of them are weekend. Are they different? For me, they are different. They have different functions, so Fiday is different from Sunday. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "printAns(lids35, loose, 5, 'text3 (edits made to fix word counts)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Korean', 'Chinese', 'Spanish'])\n"
     ]
    }
   ],
   "source": [
    "print(llang_dict_35.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa8', 'ac5', 'al5', 'an5', 'au5', 'au6', 'bu4', 'bv9', 'cc4', 'ce3', 'cj8', 'co5', 'cw1', 'dd9', 'dy7', 'fa0', 'ff1', 'fj7', 'fp5', 'fu6', 'fx4', 'gq8', 'gz2', 'ha2'], 'Chinese': ['aq1', 'ar8', 'ar9', 'az8', 'bf2', 'bl4', 'bl7', 'bp4', 'bz2', 'cb3', 'cj5', 'cz4', 'dc0', 'dm8', 'dq9', 'ei8', 'et4', 'ev6', 'ev9', 'ff2', 'fg8', 'fk8', 'gb4', 'gf5', 'gm1'], 'Spanish': ['bi4', 'bj2', 'cv7', 'dc6', 'df4', 'dk0', 'eo8', 'fa2', 'fx7', 'fy1', 'gq4', 'gu1']}\n"
     ]
    }
   ],
   "source": [
    "print(llang_dict_45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['ag9', 'an5', 'as0', 'ay1', 'be7', 'bv9', 'bz5', 'cc4', 'cj8', 'co5', 'cv3', 'cw0', 'ea4', 'eq8', 'es9', 'fj7', 'fp5', 'fu6', 'gc2', 'gq8', 'gz2', 'hb4'], 'Chinese': ['aq1', 'ar8', 'bf7', 'bl4', 'bl7', 'bp4', 'bz2', 'cb3', 'cf9', 'cz4', 'dk6', 'ev9', 'fi1', 'fj4', 'gb4', 'gx5'], 'Spanish': ['bj2', 'cm9', 'fa2', 'fy1']}\n"
     ]
    }
   ],
   "source": [
    "print(lang_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Korean': ['aa8', 'ac5', 'ad1', 'ag9', 'al5', 'an5', 'ap4', 'aq5', 'as0', 'as7', 'au5', 'au6', 'aw6', 'ay1', 'be7', 'bq0', 'bu4', 'bv9', 'bw3', 'bz5', 'cc4', 'ce3', 'ch2', 'ci0', 'cj8', 'co5', 'cp0', 'cv3', 'cw0', 'cw1', 'dd9', 'df3', 'dm3', 'dy7', 'ea4', 'eb3', 'eb9', 'ec1', 'eg5', 'eq8', 'es0', 'es9', 'et3', 'ex3', 'fa0', 'fb4', 'fd6', 'ff1', 'fi5', 'fj7', 'fl0', 'fp5', 'fs0', 'ft2', 'fu6', 'fx4', 'fz8', 'ga1', 'gc2', 'ge6', 'gg2', 'gg6', 'gj0', 'gk5', 'gl1', 'gn0', 'gq8', 'gs3', 'gu0', 'gz2', 'ha2', 'hb4'], 'Chinese': ['ad7', 'ae9', 'af4', 'am5', 'an7', 'aq1', 'ar8', 'ar9', 'az8', 'bd7', 'bf2', 'bf7', 'bf9', 'bl4', 'bl7', 'bp2', 'bp4', 'br9', 'bv8', 'bw9', 'by5', 'by6', 'bz1', 'bz2', 'ca4', 'cb3', 'cd6', 'cf9', 'ci2', 'cj5', 'cl3', 'cz2', 'cz4', 'dc0', 'di7', 'dk6', 'dm8', 'dq9', 'du9', 'dw2', 'ei2', 'ei8', 'eo1', 'eq4', 'es4', 'et4', 'ev6', 'ev9', 'ff2', 'fg8', 'fi1', 'fj4', 'fk8', 'fo4', 'fq5', 'fw7', 'ga3', 'gb4', 'gf5', 'gl2', 'gm1', 'go8', 'gt0', 'gv3', 'gw0', 'gx5'], 'Spanish': ['bi4', 'bj2', 'br2', 'cm9', 'cv7', 'dc6', 'df4', 'dk0', 'eo8', 'fa2', 'fx7', 'fy1', 'gq4', 'gs6', 'gu1', 'gu9']}\n"
     ]
    }
   ],
   "source": [
    "print(lang_dict_45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
